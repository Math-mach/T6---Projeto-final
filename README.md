# üìä Documenta√ß√£o do projeto

**Prop√≥sito do Projeto**

<small>

Este projeto foi desenvolvido para realizar um ciclo completo de an√°lise de dados, desde a limpeza e prepara√ß√£o at√© a modelagem preditiva e avalia√ß√£o de performance. O objetivo principal √© transformar um conjunto de dados brutos de jogadores em insights acion√°veis e modelos de _machine learning_ robustos, capazes de prever tr√™s m√©tricas-alvo distintas (`Target1`, `Target2` e `Target3`).

O processo √© dividido em duas fases principais:

1.  **Fase de Limpeza e Prepara√ß√£o (V1)**: Focada em garantir a qualidade e a consist√™ncia dos dados. Nesta etapa, s√£o aplicadas t√©cnicas como tratamento de valores ausentes, remo√ß√£o de _outliers_, _feature engineering_ e sele√ß√£o de vari√°veis para criar um _dataset_ otimizado e confi√°vel.

2.  **Fase de Modelagem (V2)**: Utiliza o _dataset_ limpo para treinar, avaliar e comparar diversos algoritmos de regress√£o, como _Random Forest_, _XGBoost_ e _LightGBM_. O objetivo √© identificar os modelos com melhor desempenho para cada uma das vari√°veis-alvo, salvando-os para futura implementa√ß√£o em produ√ß√£o.

Ao final, o projeto entrega n√£o apenas os modelos treinados, mas tamb√©m an√°lises visuais e relat√≥rios que facilitam a interpreta√ß√£o dos resultados e a identifica√ß√£o das vari√°veis mais influentes.

</small>

<details>

<summary> üìä Fase V1 - Limpeza</summary>

### üß© **C√©lula 1/2 - Configura√ß√£o do Ambiente e Importa√ß√µes**

<small>
üìñ Explica√ß√£o:

Esta c√©lula inicializa o ambiente de trabalho. Primeiro, realiza a instala√ß√£o silenciosa das depend√™ncias externas necess√°rias para a manipula√ß√£o de planilhas e para a aplica√ß√£o de t√©cnicas de aprendizado de m√°quina. Em seguida, importa as bibliotecas e m√≥dulos que ser√£o utilizados ao longo das etapas de limpeza, transforma√ß√£o e an√°lise de dados.

<details>

<summary> Bibliotecas utilizadas: </summary>

<small>

- **xlsxwriter e openpyxl**:

Depend√™ncias utilizadas pelo Pandas para ler e escrever arquivos no formato Excel (.xlsx). A instala√ß√£o garante a compatibilidade com essas opera√ß√µes.

- **scikit-learn**:

Uma das principais bibliotecas de aprendizado de m√°quina em Python, que fornece ferramentas eficientes para pr√©-processamento, modelagem e avalia√ß√£o de dados.

- **pandas**:

Fundamental para a manipula√ß√£o e an√°lise de dados. √â utilizada para carregar os dados em estruturas conhecidas como DataFrames, que facilitam a limpeza e a transforma√ß√£o.

- **numpy**:

Essencial para computa√ß√£o num√©rica, oferece suporte a arrays e matrizes multidimensionais, al√©m de uma vasta cole√ß√£o de fun√ß√µes matem√°ticas de alto desempenho.

- **SimpleImputer**:

Uma classe do Scikit-learn usada para tratar dados ausentes (NaN), permitindo preench√™-los com uma estrat√©gia definida (como a m√©dia, mediana ou a moda da coluna).

- **StandardScaler**:

Uma classe do Scikit-learn utilizada para padronizar as features num√©ricas, redimensionando-as para que tenham m√©dia zero e desvio padr√£o igual a um, o que √© crucial para muitos algoritmos de machine learning.

- **datetime**:

M√≥dulo padr√£o do Python para manipula√ß√£o de datas e horas.

- **warnings**:

M√≥dulo para controlar a exibi√ß√£o de mensagens de aviso. A linha warnings.filterwarnings('ignore') √© usada para suprimir esses avisos e manter a sa√≠da do c√≥digo mais limpa e focada nos resultados.
</small>

</details>

---

### üß© **C√©lula 3 - DEFINI√á√ïES**

<details>

<summary> Trecho do codigo em Python </summary>

```python
COLUNAS_CATEGORICAS = [
    'Cor0202', 'Cor0204', 'Cor0206', 'Cor0208', 'Cor0209Outro',
    'P01', 'P02', 'P03', 'P04', 'P05', 'P07', 'P08', 'P09', 'P10',
    'P12', 'P13', 'P15', 'P12_1', 'P02_1', 'P03_1', 'P09_1'
]

COLUNAS_TARGETS = ['Target1', 'Target2', 'Target3']

COLUNAS_IGNORAR = [
    'C√≥digo de Acesso', 'Data/Hora √öltimo',
    'L0210 (n√£o likert)',
    'F0299 - Explica√ß√£o Tempo', 'T0499 - Explica√ß√£o Tempo',
    'PTempoTotalExpl', 'T1199Expl', 'T1205Expl', 'T1210Expl',
    'TempoTotalExpl'
]

print(f"üìã Categ√≥ricas: {len(COLUNAS_CATEGORICAS)} | Targets: {len(COLUNAS_TARGETS)}")
```

</details>
<small> üìñ Explica√ß√£o:

Esta c√©lula centraliza as **defini√ß√µes estruturais** do dataset, classificando as colunas em grupos conforme seu papel no processamento:

- **`COLUNAS_CATEGORICAS`**: lista de vari√°veis qualitativas ou de m√∫ltipla escolha, normalmente representadas por c√≥digos (`P01`, `P02`, etc.).
- **`COLUNAS_TARGETS`**: define as vari√°veis-alvo (targets) utilizadas em an√°lises ou modelagem.
- **`COLUNAS_IGNORAR`**: cont√©m vari√°veis irrelevantes ou auxiliares, como identificadores, timestamps e campos descritivos.
  </small>

---

### üß© **C√©lula 4 - Carregando Dataset Recebido**

<details>

<summary> Trecho do codigo em Python </summary>

```python
import pandas as pd

df_original = pd.read_excel('JogadoresV1.xlsx')
df = df_original.copy()
print(f"Dados carregados: {df.shape[0]} linhas, {df.shape[1]} colunas")
```

</details>
<small>
üìñ Explica√ß√£o:

Nesta c√©lula, realizamos a leitura do arquivo Excel contendo o dataset bruto e criamos uma c√≥pia para preservar o original.

- `pd.read_excel()` carrega o arquivo no formato Excel.
- `.copy()` evita modifica√ß√µes acidentais no dataset original.  
  </small>

---

### üß© **C√©lula 5 - TRATAMENTO F0103**

<details>

<summary> Trecho do codigo em Python </summary>

```python
if 'F0103' in df.columns and df['F0103'].dtype == 'object':
    print("\nüîß Convertendo F0103 (v√≠rgula ‚Üí ponto)")
    df['F0103'] = df['F0103'].str.replace(',', '.').astype(float)
    print("   ‚úÖ Convertido!")
```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, corrigimos a coluna chamada `F0103` para garantir que os n√∫meros estejam em um formato que o Python consegue entender corretamente.

- Primeiro verificamos se a coluna `F0103` existe na tabela e se ela est√° como texto.

- Em seguida, substitu√≠mos todas as v√≠rgulas `,` por pontos `.` ‚Äî isso √© importante porque em alguns arquivos, n√∫meros decimais v√™m escritos como `3,14` em vez de `3.14`.
- Por fim, transformamos essa coluna em n√∫meros de ponto flutuante (`float`), para que possa ser usada em c√°lculos, an√°lises e modelos sem causar erros.

Esse passo garante que a informa√ß√£o da coluna `F0103` seja precisa e utiliz√°vel em todas as etapas seguintes do processamento de dados.
</small>

---

### üß© **C√©lula 6 ‚Äî REMO√á√ÉO DE NEGATIVOS**

<details>

<summary> Trecho do codigo em Python </summary>

```python
print("\n" + "=" * 80)
print("ETAPA 1: REMO√á√ÉO DE NEGATIVOS ‚Üí NaN")
print("=" * 80)

colunas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()
colunas_numericas = [col for col in colunas_numericas if col not in COLUNAS_TARGETS]

contador = 0
for col in colunas_numericas:
    negativos = (df[col] < 0).sum()
    if negativos > 0:
        df.loc[df[col] < 0, col] = np.nan
        contador += negativos

print(f"‚úÖ {contador} negativos convertidos ‚Üí NaN")

```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, identificamos todas as colunas num√©ricas e substitu√≠mos valores negativos por valores vazios (NaN).

- Primeiro, usamos `df.select_dtypes(include=[np.number])` para selecionar todas as colunas que cont√™m n√∫meros.

- Em seguida, removemos da lista as colunas-alvo definidas em `COLUNAS_TARGETS`, pois essas n√£o devem ser modificadas nesse passo.

- Para cada coluna num√©rica restante, verificamos quantos valores s√£o negativos usando `(df[col] < 0).sum()`.

- Quando valores negativos s√£o encontrados, usamos `df.loc[df[col] < 0, col] = np.nan` para substitu√≠-los.

- Ao final, imprimimos a quantidade total de valores negativos convertidos.

</small>

---

### üß© **C√©lula 7 - Remo√ß√£o de Colunas com Muito Missing**

<details>

<summary> Trecho do codigo em Python </summary>

```python
print("\n" + "=" * 80)
print("ETAPA 2: AN√ÅLISE DE MISSING")
print("=" * 80)

missing_info = pd.DataFrame({
    'Coluna': df.columns,
    'Missing': df.isna().sum(),
    'Percentual': (df.isna().sum() / len(df) * 100).round(2)
})
missing_info = missing_info[missing_info['Missing'] > 0].sort_values('Percentual', ascending=False)

threshold = 70
colunas_remover = missing_info[missing_info['Percentual'] > threshold]['Coluna'].tolist()

if colunas_remover:
    print(f"üóëÔ∏è  Removendo {len(colunas_remover)} colunas (>{threshold}% missing)")
    df = df.drop(columns=colunas_remover)

print(f"‚úÖ Shape: {df.shape}")

```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, realizamos uma an√°lise de dados faltantes (`missing values`) em cada coluna e removemos aquelas que possuem um percentual alto de aus√™ncia de dados.

- Usamos `df.isna().sum()` para contar quantos valores est√£o faltando em cada coluna.

- Calculamos o percentual de valores faltantes dividindo pela quantidade total de linhas (`len(df)`).

- Criamos o DataFrame `missing_info`, que cont√©m o nome da coluna, quantidade de valores ausentes e percentual de aus√™ncia.

- Ordenamos `missing_info` pelo percentual de aus√™ncia em ordem decrescente para focar nas colunas com mais dados faltantes.

- Definimos um limite (`threshold`) de 70%. Isso significa que qualquer coluna com mais de 70% de valores faltantes ser√° removida.

- Usamos `df.drop(columns=colunas_remover)` para eliminar essas colunas do dataset.

- Por fim, imprimimos quantas colunas foram removidas e o novo formato da tabela (`df.shape`).

</small>

---

### üß© **C√©lula 8 - Remo√ß√£o de Jogadores sem Targets**

<details>

<summary> Trecho do codigo em Python </summary>

```python
print("\n" + "=" * 80)
print("ETAPA 3: REMO√á√ÉO DE JOGADORES SEM TARGETS")
print("=" * 80)

antes = len(df)
df = df.dropna(subset=COLUNAS_TARGETS, how='all')
depois = len(df)

print(f"‚úÖ Jogadores mantidos: {depois} (removidos: {antes-depois})")

```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, garantimos que todos os registros (linhas) do dataset contenham pelo menos uma informa√ß√£o nos campos-alvo definidos em `COLUNAS_TARGETS`.

- Usamos `len(df)` para contar quantas linhas existem antes da limpeza (antes).

- O comando `df.dropna(subset=COLUNAS_TARGETS, how='all')` remove todas as linhas em que todos os campos de target estejam vazios (`NaN`).

- Calculamos novamente o tamanho do dataset (`depois`) para saber quantos registros restaram.

- Imprimimos a quantidade de jogadores mantidos e removidos ap√≥s o filtro.

</small>

---

### **üß© C√©lula 9 - Imputa√ß√£o de Valores Faltantes**

<details>

<summary> Trecho do codigo em Python </summary>

```python
# Num√©ricas: MEDIANA
colunas_num_imputar = [
    col for col in df.select_dtypes(include=[np.number]).columns
    if col not in COLUNAS_TARGETS and col not in COLUNAS_IGNORAR
]

if colunas_num_imputar:
    imputer_num = SimpleImputer(strategy='median')
    df[colunas_num_imputar] = imputer_num.fit_transform(df[colunas_num_imputar])
    print(f"‚úÖ {len(colunas_num_imputar)} num√©ricas imputadas (mediana)")

# Categ√≥ricas: MODA
colunas_cat_imputar = [col for col in COLUNAS_CATEGORICAS if col in df.columns]

if colunas_cat_imputar:
    imputer_cat = SimpleImputer(strategy='most_frequent')
    df[colunas_cat_imputar] = imputer_cat.fit_transform(df[colunas_cat_imputar])
    print(f"‚úÖ {len(colunas_cat_imputar)} categ√≥ricas imputadas (moda)")


```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, tratamos os valores faltantes no dataset usando imputa√ß√£o ‚Äî ou seja, substitu√≠mos valores ausentes (`NaN`) por valores calculados com base nos dados dispon√≠veis.

- Colunas num√©ricas:

  - Identificamos todas as colunas num√©ricas usando `df.select_dtypes(include=[np.number])`.

  - Removemos as colunas-alvo (`COLUNAS_TARGETS`) e as colunas marcadas para ignorar (`COLUNAS_IGNORAR`).

  - Criamos um imputador (`SimpleImputer`) usando a estrat√©gia `median` para substituir valores ausentes pela mediana daquela coluna.

  - Aplicamos a imputa√ß√£o usando `fit_transform`.

- Colunas categ√≥ricas:

  - Identificamos as colunas categ√≥ricas presentes (`COLUNAS_CATEGORICAS`).

  - Criamos um imputador usando a estrat√©gia `most_frequent` para substituir valores ausentes pelo valor mais frequente da coluna (moda).

  - Aplicamos a imputa√ß√£o usando fit_transform.

</small>

---

### **üß© C√©lula 10 - Tratamento de Outliers (IQR + Mediana)**

<details>

<summary> Trecho do codigo em Python </summary>

```python
colunas_outliers = [
    col for col in colunas_num_imputar
    if col not in ['QtdHorasSono', 'QtdHorasDormi', 'Acordar']
]

outliers_tratados = 0
for col in colunas_outliers:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    limite_inf = Q1 - 1.5 * IQR
    limite_sup = Q3 + 1.5 * IQR

    outliers_mask = (df[col] < limite_inf) | (df[col] > limite_sup)
    n_outliers = outliers_mask.sum()

    if n_outliers > 0:
        mediana = df[col].median()
        df.loc[outliers_mask, col] = mediana
        outliers_tratados += n_outliers

print(f"‚úÖ {outliers_tratados} outliers tratados (substitu√≠dos por mediana)")

```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, detectamos e tratamos outliers ‚Äî valores extremos que podem distorcer an√°lises e modelos ‚Äî utilizando o m√©todo do Intervalo Interquart√≠lico (IQR) e substituindo-os pela mediana da coluna.

- Primeiro identificamos as colunas num√©ricas a serem tratadas (`colunas_outliers`), excluindo colunas espec√≠ficas como `QtdHorasSono`, `QtdHorasDormi` e `Acordar`.

- Para cada coluna:

  - Calculamos o **primeiro quartil** (`Q1`) e o **terceiro quartil** (`Q3`).

  - Determinamos o **Intervalo Interquart√≠lico (IQR)** como `Q3 - Q1`.

  - Definimos limites inferior (`limite_inf`) e superior (`limite_sup`) como `Q1 - 1.5*IQR` e `Q3 + 1.5*IQR`, respectivamente.

  - Criamos uma m√°scara (`outliers_mask`) identificando valores fora desses limites.

  - Quando outliers s√£o encontrados, substitu√≠mos esses valores pela mediana da coluna.

  - Contabilizamos quantos outliers foram tratados.

</small>

---

### **üß© C√©lula 11 - One-Hot Encoding**

<details>

<summary> Trecho do codigo em Python </summary>

```python
colunas_cat_presentes = [col for col in COLUNAS_CATEGORICAS if col in df.columns]

if colunas_cat_presentes:
    colunas_antes = df.shape[1]
    df = pd.get_dummies(df, columns=colunas_cat_presentes, prefix=colunas_cat_presentes, drop_first=False)
    colunas_depois = df.shape[1]

    print(f"‚úÖ One-Hot conclu√≠do: {colunas_antes} ‚Üí {colunas_depois} colunas")

```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, aplicamos a t√©cnica de One-Hot Encoding, que transforma colunas categ√≥ricas em vari√°veis num√©ricas bin√°rias, permitindo que algoritmos de machine learning trabalhem com esses dados.

- Identificamos as colunas categ√≥ricas presentes no dataset (`colunas_cat_presentes`) comparando `COLUNAS_CATEGORICAS` com as colunas reais do `df`.

- Guardamos o n√∫mero inicial de colunas (`colunas_antes`).

- Usamos `pd.get_dummies()` para criar colunas bin√°rias para cada categoria, mantendo o prefixo original para identifica√ß√£o.

- N√£o usamos `drop_first=True` para preservar todas as categorias.

- Calculamos o n√∫mero final de colunas (`colunas_depois`) ap√≥s a transforma√ß√£o.

- Imprimimos quantas colunas foram adicionadas no processo.

</small>

---

### **üß© C√©lula 12 - Feature Engineering (Agrega√ß√µes)**

<details>

<summary> Trecho do codigo em Python </summary>

```python
# Agrega√ß√£o F11*
f11_cols = [c for c in df.columns if c.startswith('F11') and pd.api.types.is_numeric_dtype(df[c])]
if len(f11_cols) > 2:
    df['F11_mean'] = df[f11_cols].mean(axis=1)
    print(f"‚úÖ F11_mean criada ({len(f11_cols)} colunas)")

# Agrega√ß√£o F07*
f07_cols = [c for c in df.columns if c.startswith('F07') and pd.api.types.is_numeric_dtype(df[c])]
if len(f07_cols) > 2:
    df['F07_mean'] = df[f07_cols].mean(axis=1)
    print(f"‚úÖ F07_mean criada ({len(f07_cols)} colunas)")
```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, realizamos Feature Engineering, criando novas colunas que representam agrega√ß√µes de vari√°veis relacionadas, para facilitar a an√°lise e potencialmente melhorar a performance de modelos.

- **Agrega√ß√£o F11**:

  - Identificamos colunas cujo nome come√ßa com "`F11`" e que s√£o num√©ricas.

  - Se existirem mais de duas colunas nesse grupo, calculamos a m√©dia delas linha a linha (`mean(axis=1)`), criando a nova coluna `F11_mean`.

  - Exibimos quantas colunas foram utilizadas para essa agrega√ß√£o.

- **Agrega√ß√£o F07**:

  - De forma semelhante, identificamos colunas que come√ßam com "`F07`" e que s√£o num√©ricas.

  - Se houver mais de duas, calculamos a m√©dia e criamos `F07_mean`.

  - Exibimos quantas colunas contribu√≠ram para essa agrega√ß√£o.

</small>

---

### **üß© C√©lula 13 - üîß Corre√ß√£o 1 - Converter BOOL ‚Üí INT**

<details>

<summary> Trecho do codigo em Python </summary>

```python
bool_cols = df.select_dtypes(include=['bool']).columns.tolist()

if bool_cols:
    print(f"üìã Convertendo {len(bool_cols)} colunas booleanas...")
    df[bool_cols] = df[bool_cols].astype(int)
    print(f"‚úÖ VERDADEIRO/FALSO ‚Üí 1/0")
    print(f"   Exemplo: {bool_cols[:3]}")
else:
    print("‚ÑπÔ∏è  Nenhuma coluna booleana encontrada")
```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, corrigimos o formato das colunas booleanas do dataset, transformando valores `True`/`False` em `1/0`. Isso √© necess√°rio porque muitos algoritmos de machine learning requerem que todos os dados sejam num√©ricos.

- Usamos `df.select_dtypes(include=['bool'])` para identificar todas as colunas que cont√™m valores booleanos (`True` ou `False`).

- Se houver colunas booleanas (`bool_cols`), exibimos quantas ser√£o convertidas.

- Aplicamos `.astype(int)` para transformar os valores em n√∫meros inteiros (`1` para `True`, `0` para `False`).

- Exibimos alguns exemplos de colunas convertidas para confirmar a a√ß√£o.

- Caso n√£o existam colunas booleanas, uma mensagem informativa √© exibida.

</small>

---

### **üß© C√©lula 14 - Feature Selection por Correla√ß√£o**

<details>

<summary> Trecho do codigo em Python </summary>

```python
print("\n" + "=" * 80)
print("ETAPA 8: FEATURE SELECTION")
print("=" * 80)

features_numericas_finais = [
    col for col in df.columns
    if col not in COLUNAS_TARGETS
    and col not in COLUNAS_IGNORAR
    and pd.api.types.is_numeric_dtype(df[col])
]

print(f"üìä Features dispon√≠veis: {len(features_numericas_finais)}")

corr_t1 = df[features_numericas_finais].corrwith(df['Target1']).abs()
corr_t2 = df[features_numericas_finais].corrwith(df['Target2']).abs()
corr_t3 = df[features_numericas_finais].corrwith(df['Target3']).abs()

corr_mean = (corr_t1 + corr_t2 + corr_t3) / 3

threshold = 0.20
features_selecionadas = corr_mean[corr_mean > threshold].index.tolist()

print(f"‚úÖ Features mantidas: {len(features_selecionadas)} (threshold={threshold})")
print(f"‚úÖ Features removidas: {len(features_numericas_finais) - len(features_selecionadas)}")

# Top 10
print(f"\nüèÜ TOP 10 FEATURES:")
top10 = corr_mean.sort_values(ascending=False).head(10)
for idx, (feat, corr) in enumerate(top10.items(), 1):
    print(f"   {idx:2d}. {feat:30s} | Corr: {corr:.4f}")
```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, realizamos a **sele√ß√£o de features** com base na correla√ß√£o entre vari√°veis num√©ricas e os targets (`Target1`, `Target2`, `Target3`). O objetivo √© manter apenas as colunas que t√™m relev√¢ncia estat√≠stica para o modelo.

- Definimos `features_numericas_finais` como todas as colunas num√©ricas que n√£o s√£o targets nem est√£o na lista `COLUNAS_IGNORAR`.

- Calculamos a correla√ß√£o absoluta (`.abs()`) entre cada feature e cada target (`corr_t1`, `corr_t2`, `corr_t3`).

- Obtemos a m√©dia das correla√ß√µes (`corr_mean`) para avaliar a import√¢ncia geral da feature em rela√ß√£o a todos os targets.

- Definimos um **threshold** (limite) de 0.20; apenas features com correla√ß√£o m√©dia acima desse valor s√£o mantidas (`features_selecionadas`).

- Informamos quantas features foram mantidas e quantas removidas.

- Exibimos as **Top 10 features** com maior correla√ß√£o m√©dia, ordenadas do maior para o menor valor, para refer√™ncia.

</small>

---

### **üß© C√©lula 15 - Criar DataFrames Finais**

<details>

<summary> Trecho do codigo em Python </summary>

```python
df_final_nao_normalizado = df[features_selecionadas + COLUNAS_TARGETS].copy()
print(f"‚úÖ DataFrame N√ÉO-NORMALIZADO: {df_final_nao_normalizado.shape}")

df_final_normalizado = df_final_nao_normalizado.copy()

# Identificar categ√≥ricas (One-Hot) para N√ÉO normalizar
features_cat_onehot = [
    col for col in features_selecionadas
    if any(cat in col for cat in COLUNAS_CATEGORICAS)
]

features_numericas_normalizar = [
    col for col in features_selecionadas
    if col not in features_cat_onehot
]

print(f"üî¢ Num√©ricas a normalizar: {len(features_numericas_normalizar)}")
print(f"üìù Categ√≥ricas (preservadas): {len(features_cat_onehot)}")

if features_numericas_normalizar:
    scaler = StandardScaler()
    df_final_normalizado[features_numericas_normalizar] = scaler.fit_transform(
        df_final_normalizado[features_numericas_normalizar]
    )
    print(f"‚úÖ Normaliza√ß√£o conclu√≠da!")

print(f"‚úÖ DataFrame NORMALIZADO: {df_final_normalizado.shape}")
```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, preparamos os **DataFrames finais** para an√°lise e modelagem, criando vers√µes normalizadas e n√£o-normalizadas.

- Criamos `df_final_nao_normalizado` contendo apenas as **features selecionadas** e os **targets**, preservando o formato original.

- Criamos uma c√≥pia chamada `df_final_normalizado` para aplicar normaliza√ß√£o sem alterar o original.

- Identificamos colunas categ√≥ricas geradas pelo **One-Hot Encoding** (`features_cat_onehot`) para garantir que elas n√£o sejam normalizadas.

- Definimos `features_numericas_normalizar` como todas as features num√©ricas restantes.

- Informamos quantas features num√©ricas ser√£o normalizadas e quantas categ√≥ricas ser√£o preservadas.

- Aplicamos o `StandardScaler` √†s features num√©ricas, padronizando-as para m√©dia zero e desvio padr√£o igual a um.

- Exibimos o tamanho final de cada DataFrame, garantindo que ambos estejam prontos para uso posterior.

</small>

---

### **üß© C√©lula 16 - Exportar para Excel**

<details>

<summary> Trecho do codigo em Python </summary>

```python
output_file = 'Dados_Otimizados_V4.xlsx'

writer = pd.ExcelWriter(output_file, engine='xlsxwriter')
workbook = writer.book

header_format = workbook.add_format({
    'bold': True, 'text_wrap': True, 'valign': 'vcenter',
    'align': 'center', 'fg_color': '#1F4E78',
    'font_color': 'white', 'border': 1
})

title_format = workbook.add_format({
    'bold': True, 'font_size': 16,
    'fg_color': '#4472C4', 'font_color': 'white',
    'align': 'center', 'valign': 'vcenter', 'border': 2
})

# Aba 1: Dados n√£o-normalizados
df_final_nao_normalizado.to_excel(writer, sheet_name='Dados_Para_Analise', index=False, startrow=2)
worksheet1 = writer.sheets['Dados_Para_Analise']
worksheet1.merge_range('A1:Z1', 'üìã DADOS LIMPOS - Vers√£o Otimizada', title_format)
worksheet1.freeze_panes(3, 0)

# Aba 2: Dados normalizados
df_final_normalizado.to_excel(writer, sheet_name='Dados_Para_Modelo', index=False, startrow=2)
worksheet2 = writer.sheets['Dados_Para_Modelo']
worksheet2.merge_range('A1:Z1', 'üìä DADOS NORMALIZADOS - Para ML', title_format)
worksheet2.freeze_panes(3, 0)

# Aba 3: Resumo estat√≠stico
summary = df_final_nao_normalizado[COLUNAS_TARGETS].describe().T
summary.to_excel(writer, sheet_name='Resumo_Estatistico', startrow=2)
worksheet3 = writer.sheets['Resumo_Estatistico']
worksheet3.merge_range('A1:I1', 'üìä RESUMO ESTAT√çSTICO', title_format)

# Aba 4: Correla√ß√µes
correlations_df = pd.DataFrame({
    'Feature': corr_mean.index,
    'Corr_Target1': corr_t1.values,
    'Corr_Target2': corr_t2.values,
    'Corr_Target3': corr_t3.values,
    'Corr_Media': corr_mean.values,
    'Mantida': ['‚úÖ' if f in features_selecionadas else '‚ùå' for f in corr_mean.index]
}).sort_values('Corr_Media', ascending=False)

correlations_df.to_excel(writer, sheet_name='Correlacoes', index=False, startrow=2)
worksheet4 = writer.sheets['Correlacoes']
worksheet4.merge_range('A1:F1', 'üîç AN√ÅLISE DE CORRELA√á√ïES', title_format)

writer.close()

print(f"‚úÖ Arquivo '{output_file}' criado!")
```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, exportamos os resultados finais para um arquivo Excel estruturado, criando m√∫ltiplas abas para facilitar a an√°lise e utiliza√ß√£o dos dados.

- Definimos o nome do arquivo de sa√≠da como `Dados_Otimizados_V4.xlsx`.

- Criamos um escritor Excel (`pd.ExcelWriter`) usando o engine `xlsxwriter`.

- Definimos formata√ß√µes personalizadas para cabe√ßalhos (`header_format`) e t√≠tulos (`title_format`) para melhor visualiza√ß√£o.

**Aba 1 ‚Äì Dados n√£o-normalizados:**

- Exporta `df_final_nao_normalizado`.

- Inclui um t√≠tulo e congela a visualiza√ß√£o para facilitar navega√ß√£o.

**Aba 2 ‚Äì Dados normalizados:**

- Exporta `df_final_normalizado`.

- Inclui t√≠tulo e congelamento de linhas.

**Aba 3 ‚Äì Resumo estat√≠stico:**

- Cria resumo com estat√≠sticas descritivas dos targets (`describe().T`).

**Aba 4 ‚Äì Correla√ß√µes:**

- Exporta tabela contendo correla√ß√µes m√©dias entre features e targets.

- Indica quais features foram mantidas na sele√ß√£o.

- Fecha o arquivo Excel (`writer.close()`) e confirma a cria√ß√£o.

Essa celula entrega um arquivo organizado e documentado, pronto para an√°lise e uso em modelos de machine learning.

</small>

---

### ‚úÖ **Resumo Geral da Etapa de Limpeza**

Ap√≥s a execu√ß√£o de todas as c√©lulas, o dataset estar√° pronto para ser utilizado nas pr√≥ximas fases do projeto, com:

- C√≥digos inv√°lidos convertidos em `NaN`.
- Colunas pouco informativas removidas.
- Targets completos e consistentes.
- Estrutura final reduzida, por√©m mais confi√°vel e analis√°vel.

---

</details>

<details>

<summary> üìä Fase V2 - Modelagem </summary>

### **üß© C√©lula 1 - Instala√ß√£o de Bibliotecas**

<details>

<summary> Trecho do codigo em Python </summary>

```python
!pip install xgboost lightgbm catboost scikit-learn pandas matplotlib seaborn plotly -q
print("‚úÖ Bibliotecas instaladas com sucesso!")
```

</details>

<small> üìñ Explica√ß√£o:

Esta c√©lula instala todas as bibliotecas necess√°rias para rodar o projeto de modelagem avan√ßada e visualiza√ß√µes.

O comando `!pip install` funciona dentro de notebooks Jupyter ou Google Colab e serve para instalar pacotes Python diretamente no ambiente.  
Aqui, estamos instalando:

- **xgboost, lightgbm, catboost** ‚Üí algoritmos de aprendizado de m√°quina muito eficientes para regress√£o e classifica√ß√£o.
- **scikit-learn** ‚Üí biblioteca com ferramentas para pr√©-processamento, modelagem e avalia√ß√£o de dados.
- **pandas** ‚Üí manipula√ß√£o e an√°lise de dados em tabelas.
- **matplotlib, seaborn** ‚Üí gera√ß√£o de gr√°ficos e visualiza√ß√µes.
- **plotly** ‚Üí cria√ß√£o de gr√°ficos interativos.

</small>

---

### **üß© C√©lula 2 - Importa√ß√£o de Bibliotecas**

<details>

<summary> Trecho do codigo em Python </summary>

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostRegressor
import pickle

warnings.filterwarnings('ignore')
sns.set_style('whitegrid')

print("\n" + "=" * 80)
print("     FASE 3 COMPLETA: MODELAGEM AVAN√áADA + VISUALIZA√á√ïES")
print("=" * 80)

```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, carregamos todas as bibliotecas necess√°rias para manipula√ß√£o, an√°lise, modelagem e visualiza√ß√£o dos dados.

- **pandas** e **numpy**: manipula√ß√£o e c√°lculo de dados.
- **matplotlib.pyplot** e **seaborn**: cria√ß√£o de gr√°ficos est√°ticos.
- **warnings**: para suprimir mensagens de aviso indesejadas.
- **sklearn.model_selection**: fun√ß√µes para divis√£o dos dados e valida√ß√£o cruzada.
- **sklearn.linear_model**: modelos de regress√£o Linear e Ridge.
- **sklearn.ensemble**: algoritmos ensemble como Random Forest e Gradient Boosting.
- **sklearn.metrics**: c√°lculo de m√©tricas de avalia√ß√£o como RMSE e R¬≤.
- **xgboost, lightgbm, catboost**: algoritmos de machine learning de alta performance.
- **pickle**: salvar e carregar modelos treinados.

As √∫ltimas linhas configuram o estilo dos gr√°ficos (`sns.set_style('whitegrid')`) e imprimem um t√≠tulo indicando o in√≠cio da fase 3.

</small>

---

### **üß© C√©lula 3 - Carregamento dos Dados**

<details>

<summary> Trecho do codigo em Python </summary>

```python
df = pd.read_excel('Dados_para_modelo.xlsx')

print(f"‚úÖ Dados carregados com sucesso. Shape: {df.shape}")
print(f"   Total de Jogadores: {len(df)}")
print(f"   Total de Colunas: {len(df.columns)}")

```

</details>

<small> üìñ Explica√ß√£o:

Esta c√©lula carrega os dados de um arquivo Excel chamado `'Dados_para_modelo.xlsx'` usando a biblioteca **pandas** e armazena em um DataFrame chamado `df`.

O DataFrame √© uma estrutura de dados semelhante a uma tabela, muito utilizada em an√°lise de dados.

- `df.shape` retorna uma tupla (n√∫mero de linhas, n√∫mero de colunas) para verificar o tamanho do dataset.
- `len(df)` retorna o n√∫mero total de linhas, representando a quantidade de jogadores.
- `len(df.columns)` retorna o n√∫mero total de colunas, representando as vari√°veis dispon√≠veis.

Essas impress√µes garantem que os dados foram carregados corretamente antes de prosseguir.

</small>

---

### **üß© C√©lula 4 - Separa√ß√£o de Features (X) e Targets (y)**

<details>

<summary> Trecho do codigo em Python </summary>

```python
targets = ['Target1', 'Target2', 'Target3']
X = df.drop(columns=targets)
y1 = df['Target1']
y2 = df['Target2']
y3 = df['Target3']

print(f"‚úÖ Features (X) separadas. Total de features: {X.shape[1]}")
print(f"‚úÖ Targets (y1, y2, y3) separados.")
```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula, o dataset √© separado em duas partes principais:

- **Features (X)**: as vari√°veis de entrada que ser√£o usadas para prever algo.
- **Targets (y)**: as vari√°veis que queremos prever.

No c√≥digo:

- `targets` √© uma lista com os nomes das colunas alvo (`Target1`, `Target2`, `Target3`).
- `X` cont√©m todas as colunas exceto as targets, obtido com `df.drop(columns=targets)`.
- `y1`, `y2` e `y3` cont√™m cada uma das targets separadamente.

</small>

---

### **üß© C√©lula 5 - Divis√£o em Dados de Treino e Teste (80/20)**

<details>

<summary> Trecho do codigo em Python </summary>

```python
X_train, X_test, y1_train, y1_test = train_test_split(X, y1, test_size=0.2, random_state=42)
_, _, y2_train, y2_test = train_test_split(X, y2, test_size=0.2, random_state=42)
_, _, y3_train, y3_test = train_test_split(X, y3, test_size=0.2, random_state=42)

print(f"‚úÖ Dados divididos em 80% treino e 20% teste.")
print(f"   Tamanho do treino: {len(X_train)} jogadores")
print(f"   Tamanho do teste:  {len(X_test)} jogadores")
```

</details>

<small> üìñ Explica√ß√£o:

Esta c√©lula divide os dados em conjuntos de treino e teste usando a fun√ß√£o `train_test_split` do **scikit-learn**.

- `test_size=0.2` significa que 20% dos dados ser√£o usados para teste e 80% para treino.
- `random_state=42` garante que a divis√£o seja reproduz√≠vel (sempre igual).

Para cada target (`y1`, `y2`, `y3`), s√£o criados conjuntos separados:

- `X_train`, `X_test`: dados de entrada para treino e teste.
- `y1_train`, `y1_test`, etc.: valores alvo correspondentes.

</small>

---

### **üß© C√©lula 6 - Defini√ß√£o dos Modelos a Serem Testados**

<details>

<summary> Trecho do codigo em Python </summary>

```python
modelos = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42),
    'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1),
    'LightGBM': lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1, verbose=-1),
    'CatBoost': CatBoostRegressor(iterations=100, depth=6, learning_rate=0.1, random_state=42, verbose=False)
}
print(f"‚úÖ {len(modelos)} modelos definidos para teste.")
```

</details>

<small> üìñ Explica√ß√£o:

Nesta c√©lula definimos um dicion√°rio chamado `modelos` contendo v√°rios algoritmos de machine learning para serem testados no projeto.

Cada chave √© o nome do modelo e cada valor √© uma inst√¢ncia do modelo com par√¢metros definidos:

- **Linear Regression** e **Ridge**: modelos lineares b√°sicos.
- **Random Forest**: modelo ensemble baseado em √°rvores, com par√¢metros como `n_estimators` (n√∫mero de √°rvores) e `max_depth` (profundidade m√°xima).
- **Gradient Boosting**: modelo ensemble que ajusta sequencialmente as √°rvores para reduzir erros.
- **XGBoost, LightGBM, CatBoost**: algoritmos avan√ßados e muito eficientes para regress√£o, com par√¢metros como `learning_rate`, `max_depth` e n√∫mero de itera√ß√µes (`n_estimators` ou `iterations`).

</small>

---

### **üß© C√©lula 7 - Fun√ß√£o de Treinamento e Avalia√ß√£o**

<details>

<summary> Trecho do codigo em Python </summary>

```python
def treinar_avaliar_modelo(modelo, X_train, X_test, y_train, y_test):
    """Fun√ß√£o para treinar, prever e avaliar um modelo, retornando as m√©tricas e o modelo treinado."""
    modelo.fit(X_train, y_train)
    y_pred = modelo.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    return {'modelo': modelo, 'y_pred': y_pred, 'r2': r2, 'rmse': rmse, 'mae': mae}

```

</details>

<small> üìñ Explica√ß√£o:

Esta c√©lula define uma fun√ß√£o chamada `treinar_avaliar_modelo` que serve para treinar um modelo de machine learning e avaliar seu desempenho.

Par√¢metros da fun√ß√£o:

- `modelo`: objeto do modelo a ser treinado.
- `X_train`, `X_test`: dados de entrada para treino e teste.
- `y_train`, `y_test`: valores alvo para treino e teste.

O processo realizado dentro da fun√ß√£o:

1. `modelo.fit(X_train, y_train)` ‚Üí treina o modelo com os dados de treino.
2. `modelo.predict(X_test)` ‚Üí faz previs√µes com os dados de teste.
3. Calcula m√©tricas de avalia√ß√£o:
   - **R¬≤ (r2_score)**: mede a qualidade da previs√£o (quanto mais pr√≥ximo de 1, melhor).
   - **RMSE (root mean squared error)**: erro m√©dio quadr√°tico.
   - **MAE (mean absolute error)**: erro absoluto m√©dio.

A fun√ß√£o retorna um dicion√°rio com o modelo treinado, previs√µes e m√©tricas calculadas.

</small>

---

### **üß© C√©lula 8 - Treinamento e Avalia√ß√£o de Todos os Modelos**

<details>

<summary> Trecho do codigo em Python </summary>

```python
print("\n" + "=" * 80)
print("ETAPA 5: TREINAMENTO E AVALIA√á√ÉO DOS MODELOS")
print("=" * 80)

# --- Target 1 ---
resultados_t1 = {}
for nome, modelo in modelos.items():
    resultados_t1[nome] = treinar_avaliar_modelo(type(modelo)(**modelo.get_params()), X_train, X_test, y1_train, y1_test)
melhor_t1 = max(resultados_t1.items(), key=lambda x: x[1]['r2'])
print(f"üéØ Target 1 | Melhor Modelo: {melhor_t1[0]:<20} | R¬≤ = {melhor_t1[1]['r2']:.4f}")

# --- Target 2 ---
resultados_t2 = {}
for nome, modelo in modelos.items():
    resultados_t2[nome] = treinar_avaliar_modelo(type(modelo)(**modelo.get_params()), X_train, X_test, y2_train, y2_test)
melhor_t2 = max(resultados_t2.items(), key=lambda x: x[1]['r2'])
print(f"üéØ Target 2 | Melhor Modelo: {melhor_t2[0]:<20} | R¬≤ = {melhor_t2[1]['r2']:.4f}")

# --- Target 3 ---
resultados_t3 = {}
for nome, modelo in modelos.items():
    resultados_t3[nome] = treinar_avaliar_modelo(type(modelo)(**modelo.get_params()), X_train, X_test, y3_train, y3_test)
melhor_t3 = max(resultados_t3.items(), key=lambda x: x[1]['r2'])
print(f"üéØ Target 3 | Melhor Modelo: {melhor_t3[0]:<20} | R¬≤ = {melhor_t3[1]['r2']:.4f}")
```

</details>

<small> üìñ Explica√ß√£o:

Esta c√©lula realiza o treinamento e avalia√ß√£o de todos os modelos definidos para cada target (Target1, Target2, Target3).

O processo √© feito em tr√™s blocos:

1. Para cada target, criamos um dicion√°rio (`resultados_t1`, `resultados_t2`, `resultados_t3`) para armazenar os resultados.
2. Usamos um loop `for` para percorrer cada modelo definido no dicion√°rio `modelos`.
   - `type(modelo)(**modelo.get_params())` cria uma nova inst√¢ncia do modelo com os mesmos par√¢metros.
   - Chamamos a fun√ß√£o `treinar_avaliar_modelo` para treinar e avaliar o modelo.
3. Usamos `max(..., key=lambda x: x[1]['r2'])` para selecionar o modelo com melhor R¬≤ para cada target.

Ao final, imprimimos o nome do melhor modelo e seu R¬≤ para cada target.  
Isso ajuda a identificar qual modelo performou melhor para cada vari√°vel alvo.

</small>

---

### **üß© C√©lula 9 - Visualiza√ß√£o 1: Previsto vs. Real (Gr√°fico de Dispers√£o)**

<details>

<summary> Trecho do codigo em Python </summary>

```python
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
fig.suptitle('An√°lise de Previs√£o vs. Valor Real para os Melhores Modelos', fontsize=16, fontweight='bold')

# Gr√°fico para Target 1
y1_pred = melhor_t1[1]['y_pred']
axes[0].scatter(y1_test, y1_pred, alpha=0.7, color='blue', edgecolors='k')
axes[0].plot([y1_test.min(), y1_test.max()], [y1_test.min(), y1_test.max()], 'r--', lw=2, label='Linha Perfeita')
axes[0].set_xlabel('Valores Reais', fontsize=12)
axes[0].set_ylabel('Valores Previstos', fontsize=12)
axes[0].set_title(f'Target 1 - {melhor_t1[0]}\nR¬≤={melhor_t1[1]["r2"]:.3f}', fontsize=14)
axes[0].legend()
axes[0].grid(True)

# Gr√°fico para Target 2
y2_pred = melhor_t2[1]['y_pred']
axes[1].scatter(y2_test, y2_pred, alpha=0.7, color='green', edgecolors='k')
axes[1].plot([y2_test.min(), y2_test.max()], [y2_test.min(), y2_test.max()], 'r--', lw=2, label='Linha Perfeita')
axes[1].set_xlabel('Valores Reais', fontsize=12)
axes[1].set_ylabel('Valores Previstos', fontsize=12)
axes[1].set_title(f'Target 2 - {melhor_t2[0]}\nR¬≤={melhor_t2[1]["r2"]:.3f}', fontsize=14)
axes[1].legend()
axes[1].grid(True)

# Gr√°fico para Target 3
y3_pred = melhor_t3[1]['y_pred']
axes[2].scatter(y3_test, y3_pred, alpha=0.7, color='purple', edgecolors='k')
axes[2].plot([y3_test.min(), y3_test.max()], [y3_test.min(), y3_test.max()], 'r--', lw=2, label='Linha Perfeita')
axes[2].set_xlabel('Valores Reais', fontsize=12)
axes[2].set_ylabel('Valores Previstos', fontsize=12)
axes[2].set_title(f'Target 3 - {melhor_t3[0]}\nR¬≤={melhor_t3[1]["r2"]:.3f}', fontsize=14)
axes[2].legend()
axes[2].grid(True)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.savefig('grafico_dispersao_previsto_vs_real.png', dpi=300, bbox_inches='tight')
print("‚úÖ Gr√°fico de Dispers√£o (Previsto vs. Real) salvo como 'grafico_dispersao_previsto_vs_real.png'")
```

</details>

<small> üìñ Explica√ß√£o:

Esta c√©lula cria gr√°ficos de dispers√£o comparando valores previstos pelos melhores modelos com os valores reais para cada target (Target1, Target2, Target3).

O processo inclui:

- Cria√ß√£o de uma figura com tr√™s subplots (`plt.subplots(1, 3, figsize=(18, 5))`).
- Para cada target:
  - Plotar valores reais (`y_test`) vs. valores previstos (`y_pred`) usando `scatter()`.
  - Adicionar uma linha pontilhada (`plot()`) representando a previs√£o perfeita (quando previsto = real).
  - Configurar t√≠tulo, r√≥tulos e legenda.
- `plt.tight_layout()` ajusta o espa√ßamento entre gr√°ficos.
- `plt.savefig()` salva a figura como `'grafico_dispersao_previsto_vs_real.png'`.

Esse tipo de gr√°fico ajuda a visualizar a precis√£o do modelo e identificar padr√µes ou desvios.

</small>

---

### **üß© C√©lula 10 - Visualiza√ß√£o 2: Import√¢ncia das Features**

<details>

<summary> Trecho do codigo em Python </summary>

```python
def plotar_importancia(melhor_modelo_info, target_name, feature_names, ax):
    """Fun√ß√£o auxiliar para plotar a import√¢ncia das features em um eixo do matplotlib."""
    nome_modelo = melhor_modelo_info[0]
    modelo = melhor_modelo_info[1]['modelo']

    if hasattr(modelo, 'feature_importances_'):
        importances = modelo.feature_importances_
    else: # CatBoost
        importances = modelo.get_feature_importance()

    df_importances = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values('importance', ascending=True).tail(15)

    ax.barh(df_importances['feature'], df_importances['importance'], color='darkcyan')
    ax.set_title(f'Top 15 Features - {target_name}\n(Modelo: {nome_modelo})', fontsize=14)
    ax.set_xlabel('Import√¢ncia')

fig, axes = plt.subplots(1, 3, figsize=(20, 8))
fig.suptitle('An√°lise de Import√¢ncia das Features para os Melhores Modelos', fontsize=16, fontweight='bold')

plotar_importancia(melhor_t1, 'Target 1', X.columns, axes[0])
plotar_importancia(melhor_t2, 'Target 2', X.columns, axes[1])
plotar_importancia(melhor_t3, 'Target 3', X.columns, axes[2])

plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.savefig('grafico_feature_importance.png', dpi=300, bbox_inches='tight')
print("‚úÖ Gr√°fico de Import√¢ncia das Features salvo como 'grafico_feature_importance.png'")
```

</details>

<small> üìñ Explica√ß√£o:

Esta c√©lula cria gr√°ficos mostrando a import√¢ncia das features para os melhores modelos de cada target.

O processo inclui:

- Defini√ß√£o da fun√ß√£o `plotar_importancia()`, que:

  - Recebe informa√ß√µes do melhor modelo (`melhor_modelo_info`), o nome do target, os nomes das features e um eixo (`ax`) para plotagem.
  - Verifica se o modelo possui atributo `feature_importances_` (m√©todo comum em modelos de √°rvore). Caso seja CatBoost, usa `get_feature_importance()`.
  - Cria um DataFrame com nomes e import√¢ncias das features, ordenando e selecionando as 15 mais importantes.
  - Plota um gr√°fico de barras horizontais (`barh`).

- Cria√ß√£o de uma figura com tr√™s subplots para cada target.
- Chamadas da fun√ß√£o `plotar_importancia` para cada target.
- Ajuste de layout e salvamento do gr√°fico como `'grafico_feature_importance.png'`.

Esses gr√°ficos ajudam a entender quais vari√°veis t√™m maior influ√™ncia na previs√£o do modelo.

</small>

---

### **üß© C√©lula 11 - Salvando os Melhores Modelos**

<details>

<summary> Trecho do codigo em Python </summary>

```python
with open('modelo_target1_final.pkl', 'wb') as f: pickle.dump(melhor_t1[1]['modelo'], f)
print(f"‚úÖ Modelo para Target 1 ({melhor_t1[0]}) salvo como 'modelo_target1_final.pkl'")

with open('modelo_target2_final.pkl', 'wb') as f: pickle.dump(melhor_t2[1]['modelo'], f)
print(f"‚úÖ Modelo para Target 2 ({melhor_t2[0]}) salvo como 'modelo_target2_final.pkl'")

with open('modelo_target3_final.pkl', 'wb') as f: pickle.dump(melhor_t3[1]['modelo'], f)
print(f"‚úÖ Modelo para Target 3 ({melhor_t3[0]}) salvo como 'modelo_target3_final.pkl'")
```

</details>

<small> üìñ Explica√ß√£o:

Esta c√©lula salva os melhores modelos encontrados para cada target usando a biblioteca **pickle**.

O processo:

- Para cada target, abrimos um arquivo `.pkl` em modo de escrita bin√°ria (`'wb'`).
- Usamos `pickle.dump()` para salvar o modelo treinado (`melhor_tX[1]['modelo']`).
- Cada arquivo recebe um nome correspondente ao target (`modelo_target1_final.pkl`, etc.).
- Mensagens confirmam que os modelos foram salvos com sucesso.

Esses arquivos `.pkl` podem ser carregados posteriormente para fazer previs√µes sem precisar treinar novamente o modelo.

</small>

---

### **üß© C√©lula 12 - Relat√≥rio Final dos Resultados**

<details>

<summary> Trecho do codigo em Python </summary>

```python
print("\n" + "=" * 80)
print("üéâ FASE 3 COMPLETA - RELAT√ìRIO FINAL üéâ")
print("=" * 80)

print("\nüìä RESUMO DOS MELHORES MODELOS:\n")

print(f"  TARGET 1")
print(f"  - Melhor Modelo: {melhor_t1[0]}")
print(f"  - R¬≤ (R-quadrado): {melhor_t1[1]['r2']:.4f}  (Explica ~{melhor_t1[1]['r2']:.1%} da vari√¢ncia)")
print(f"  - RMSE (Erro M√©dio): {melhor_t1[1]['rmse']:.2f} pontos")
print(f"  - MAE (Erro Absoluto M√©dio): {melhor_t1[1]['mae']:.2f} pontos\n")

print(f"  TARGET 2")
print(f"  - Melhor Modelo: {melhor_t2[0]}")
print(f"  - R¬≤ (R-quadrado): {melhor_t2[1]['r2']:.4f}  (Explica ~{melhor_t2[1]['r2']:.1%} da vari√¢ncia)")
print(f"  - RMSE (Erro M√©dio): {melhor_t2[1]['rmse']:.2f} pontos")
print(f"  - MAE (Erro Absoluto M√©dio): {melhor_t2[1]['mae']:.2f} pontos\n")

print(f"  TARGET 3")
print(f"  - Melhor Modelo: {melhor_t3[0]}")
print(f"  - R¬≤ (R-quadrado): {melhor_t3[1]['r2']:.4f}  (Explica ~{melhor_t3[1]['r2']:.1%} da vari√¢ncia)")
print(f"  - RMSE (Erro M√©dio): {melhor_t3[1]['rmse']:.2f} pontos")
print(f"  - MAE (Erro Absoluto M√©dio): {melhor_t3[1]['mae']:.2f} pontos\n")

print("üìÅ ARQUIVOS GERADOS:")
print("  ‚úÖ modelo_target1_final.pkl")
print("  ‚úÖ modelo_target2_final.pkl")
print("  ‚úÖ modelo_target3_final.pkl")
print("  ‚úÖ grafico_dispersao_previsto_vs_real.png")
print("  ‚úÖ grafico_feature_importance.png")

print("\nüöÄ PR√ìXIMOS PASSOS:")
print("  1. Usar os arquivos '.pkl' salvos para carregar os modelos no seu backend (Node.js/FastAPI).")
print("  2. Criar as rotas da API que recebem novos dados de jogadores e usam os modelos para prever os targets.")
print("  3. Desenvolver o dashboard interativo que consome essa API e exibe os resultados e insights.")
print("  4. Preparar a apresenta√ß√£o de slides contando a hist√≥ria do projeto, dos dados aos resultados.")

print("\n‚ú® Excelente trabalho! A etapa de modelagem e an√°lise est√° conclu√≠da. ‚ú®")
```

</details>

<small> üìñ Explica√ß√£o:

Esta c√©lula gera um relat√≥rio final resumindo os resultados obtidos na fase de modelagem.

O conte√∫do inclui:

- **Resumo dos Melhores Modelos** para cada target, exibindo:
  - Nome do modelo com melhor performance.
  - R¬≤ (R-quadrado): mede a qualidade da previs√£o.
  - RMSE: erro m√©dio quadr√°tico.
  - MAE: erro absoluto m√©dio.
- **Lista dos arquivos gerados** no processo, incluindo modelos `.pkl` e gr√°ficos.
- **Pr√≥ximos passos sugeridos**, como integrar os modelos salvos a uma API e criar dashboards interativos.

</small>

---

</details>

---

**üìà Vis√£o Geral dos Resultados da Modelagem**

<small>
Nesta fase, o dataset limpo e otimizado foi utilizado para treinar e avaliar **7 algoritmos de regress√£o diferentes**, com o objetivo de encontrar o melhor modelo para prever cada uma das tr√™s m√©tricas-alvo.

Ap√≥s o treinamento e a valida√ß√£o, os modelos com melhor desempenho, medido pelo coeficiente de determina√ß√£o (R¬≤), foram:

- **Target 1**: **CatBoost**, com **R¬≤ = 0.577**. Isso indica que o modelo consegue explicar aproximadamente **57,7%** da varia√ß√£o nos dados.
- **Target 2**: **Random Forest**, com **R¬≤ = 0.406**, explicando cerca de **40,6%** da vari√¢ncia.
- **Target 3**: **Random Forest**, com **R¬≤ = 0.420**, explicando aproximadamente **42,0%** da vari√¢ncia.

A imagem abaixo apresenta uma an√°lise visual da performance desses tr√™s modelos. Cada gr√°fico de dispers√£o compara os **valores reais** (eixo X) com os **valores previstos** pelo modelo (eixo Y). A "Linha Perfeita" (tracejada em vermelho) representa o cen√°rio ideal, onde a previs√£o √© exatamente igual ao valor real.

Quanto mais pr√≥ximos os pontos estiverem dessa linha, mais precisas s√£o as previs√µes do modelo. Essa visualiza√ß√£o ajuda a entender rapidamente a capacidade preditiva dos modelos escolhidos.
</small>

![alt text](data/processed/02_model_ready/grafico_dispersao_previsto_vs_real.png)
