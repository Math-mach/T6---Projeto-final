
# ğŸ§  Projeto Daruma â€” PrevisÃ£o de Targets de Jogadores

Este projeto foi desenvolvido como soluÃ§Ã£o para o **Desafio Final de Ciclo**, com o objetivo de construir um pipeline completo de **Machine Learning** capaz de prever **3 targets numÃ©ricos** com base em dados de jogadores.

O sistema inclui:
- Pipeline automatizado de prÃ©-processamento, engenharia de features e modelagem.
- Treinamento e otimizaÃ§Ã£o de modelos via **CatBoost + Optuna**.
- ExportaÃ§Ã£o dos artefatos para integraÃ§Ã£o com **API FastAPI**.
- Arquitetura modular, eficiente e reprodutÃ­vel.

---

## âš™ï¸ Estrutura do Pipeline

O fluxo de execuÃ§Ã£o segue etapas sequenciais e independentes:

```
[ImportaÃ§Ã£o e limpeza] â†’ [Engenharia de features] â†’  
[SeleÃ§Ã£o de variÃ¡veis] â†’ [OtimizaÃ§Ã£o de hiperparÃ¢metros] â†’  
[Treinamento com ensemble] â†’ [ValidaÃ§Ã£o cruzada e mÃ©tricas] â†’ [ExportaÃ§Ã£o dos modelos]
```

Bibliotecas principais:
- **Pandas / NumPy** â€” Processamento vetorizado de alta performance  
- **Scikit-learn** â€” PadronizaÃ§Ã£o, mÃ©tricas e cross-validation  
- **Optuna** â€” OtimizaÃ§Ã£o bayesiana de hiperparÃ¢metros  
- **CatBoost** â€” Gradient boosting robusto e eficiente  
- **Matplotlib / Seaborn** â€” VisualizaÃ§Ã£o e diagnÃ³stico  
- **Joblib / Pickle** â€” SerializaÃ§Ã£o de modelos e artefatos  

---

## ğŸ§¹ Limpeza e Tratamento de Dados

A etapa inicial garante integridade e consistÃªncia dos dados:

- ConversÃ£o explÃ­cita de tipos (`pd.to_numeric(errors='coerce')`)  
- PadronizaÃ§Ã£o de separadores decimais (`,` â†’ `.`)  
- ImputaÃ§Ã£o de valores ausentes por **mediana**, robusta a outliers  
- SubstituiÃ§Ã£o de valores invÃ¡lidos (ex: `-1`) por `NaN`  

> **Justificativa:** A imputaÃ§Ã£o por mediana e o processamento vetorizado tornam o pipeline mais rÃ¡pido e confiÃ¡vel, evitando falhas e distorÃ§Ãµes estatÃ­sticas.

---

## ğŸ§© Engenharia de Features

ApÃ³s normalizar os dados, novas variÃ¡veis sÃ£o criadas para capturar padrÃµes complexos:

- EstatÃ­sticas por grupo: `mean`, `std`, `max`  
- Features temporais: mÃ©dias de perÃ­odos iniciais e tardios (`P_early`, `P_late`)  
- InteraÃ§Ãµes multiplicativas: ex. `F1103_X_P_mean`  
- CriaÃ§Ã£o condicional de features (sÃ³ se a coluna existir)  

> **BenefÃ­cio:** Amplia o poder preditivo dos modelos de boosting ao capturar relaÃ§Ãµes nÃ£o lineares.

---

## ğŸ¯ SeleÃ§Ã£o de Features

As features sÃ£o filtradas por **correlaÃ§Ã£o absoluta com o target**:

```python
corr = abs(df[col].corr(df[TARGET]))
if corr > 0.35:
    feature_pool.append(col)
```

Apenas as **15 variÃ¡veis mais relevantes** sÃ£o mantidas.

> **Vantagens:**
> - Reduz dimensionalidade e custo computacional  
> - Evita colinearidade e overfitting  
> - Melhora interpretabilidade e tempo de inferÃªncia  

---

## ğŸ¤– Modelagem e OtimizaÃ§Ã£o

Modelo base: **CatBoostRegressor**

ConfiguraÃ§Ãµes:
- OtimizaÃ§Ã£o com **Optuna** (atÃ© 100 trials)
- **ValidaÃ§Ã£o cruzada k-fold (k=3)**
- **RobustScaler** para normalizaÃ§Ã£o
- **Ensemble de 3 modelos** com seeds diferentes (`42`, `123`, `456`)

> **Motivos tÃ©cnicos:**
> - CatBoost dispensa one-hot encoding, otimizando memÃ³ria e tempo  
> - Optuna com otimizaÃ§Ã£o bayesiana Ã© mais eficiente que grid search  
> - Ensemble reduz variÃ¢ncia e melhora generalizaÃ§Ã£o  

---

## ğŸ“ ValidaÃ§Ã£o e MÃ©tricas

MÃ©tricas principais:
- **RÂ² (R-squared)** â€” variÃ¢ncia explicada  
- **MAE** â€” erro absoluto mÃ©dio  
- **Safe MAPE** â€” evita divisÃµes por zero:

```python
def safe_mape(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-10))) * 100
```

MÃ©todos adicionais:
- **Leave-One-Out Cross Validation (LOO-CV)**  
- Controle de overfitting: diferenÃ§a < 15% entre treino e teste  

---

## ğŸ’¾ ExportaÃ§Ã£o e IntegraÃ§Ã£o

ApÃ³s o treinamento:
- Modelos salvos em **.cbm** (formato nativo CatBoost)
- Scalers e features exportados com **pickle**
- IntegraÃ§Ã£o com **FastAPI** via endpoints REST

> O formato `.cbm` reduz latÃªncia e facilita o deploy em containers Docker.

---

## âš¡ EficiÃªncia e Escalabilidade

| TÃ©cnica | BenefÃ­cio |
|----------|------------|
| VetorizaÃ§Ã£o com Pandas/Numpy | AtÃ© **50Ã— mais rÃ¡pido** que iteraÃ§Ã£o |
| CatBoost + Optuna | Busca de parÃ¢metros inteligente e veloz |
| Ensemble de modelos | Reduz variÃ¢ncia e aumenta estabilidade |
| PersistÃªncia de artefatos | Evita reprocessamento e acelera inicializaÃ§Ã£o |

> O pipeline treina em **minutos** e infere em **milissegundos**, ideal para produÃ§Ã£o.

---

## ğŸ” Reprodutibilidade e ManutenÃ§Ã£o

- Seeds fixos para resultados determinÃ­sticos  
- Estrutura modular facilita manutenÃ§Ã£o e ajustes  
- Logging detalhado e mÃ©tricas salvas em tempo real  
- DocumentaÃ§Ã£o integrada ao cÃ³digo  

---

## ğŸ ConclusÃ£o

O **Projeto Daruma** entrega um pipeline completo de Machine Learning:
- **CatBoost + Optuna â†’ precisÃ£o e eficiÃªncia**
- **Ensemble + LOO-CV â†’ estabilidade e confiabilidade**
- **Safe MAPE + RobustScaler â†’ mÃ©tricas consistentes**
- **ExportaÃ§Ã£o modular â†’ integraÃ§Ã£o Ã¡gil com APIs e dashboards**

> Este pipeline representa um **padrÃ£o de referÃªncia em engenharia de ML para dados tabulares** â€” eficiente, escalÃ¡vel e reprodutÃ­vel.