This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
auth.py
catboost_info/catboost_training.json
catboost_info/learn_error.tsv
catboost_info/test_error.tsv
catboost_info/time_left.tsv
core.py
crud.py
database.py
Dockerfile
export_artifacts_target1.py
export_artifacts_target2.py
export_artifacts_target3.py
export_clustering_artifacts.py
export_hibrido_target1.py
export_hibrido_target2.py
export_hibrido_target3.py
main.py
models.py
README.md
requirements.txt
schemas.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="auth.py">
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from datetime import datetime, timedelta
import os
from core import bcrypt # <--- IMPORTAÇÃO CHAVE

SECRET_KEY = os.getenv('JWT_SECRET_KEY', 'default_secret_key')
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="login")

def verify_password(plain_password, hashed_password):
    """Verifica a senha usando bcrypt."""
    # A biblioteca Flask-Bcrypt espera que o hash seja um bytes-like object
    return bcrypt.check_password_hash(hashed_password.encode('utf-8'), plain_password)

def get_password_hash(password):
    """Gera o hash da senha usando bcrypt."""
    return bcrypt.generate_password_hash(password).decode('utf-8')

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

async def get_current_user_id(token: str = Depends(oauth2_scheme)):
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        user_id: str = payload.get("sub")
        if user_id is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
    return user_id
</file>

<file path="catboost_info/catboost_training.json">
{
"meta":{"test_sets":[],"test_metrics":[],"learn_metrics":[{"best_value":"Min","name":"RMSE"}],"launch_mode":"Train","parameters":"","iteration_count":338,"learn_sets":["learn"],"name":"experiment"},
"iterations":[
{"learn":[22.62912629],"iteration":0,"passed_time":0.0005901227796,"remaining_time":0.1988713767},
{"learn":[22.55904783],"iteration":1,"passed_time":0.001105893137,"remaining_time":0.1857900469},
{"learn":[22.45396966],"iteration":2,"passed_time":0.001690344431,"remaining_time":0.1887551282},
{"learn":[22.34197654],"iteration":3,"passed_time":0.002289801272,"remaining_time":0.1911984062},
{"learn":[22.26364277],"iteration":4,"passed_time":0.002876551776,"remaining_time":0.1915783483},
{"learn":[22.18266737],"iteration":5,"passed_time":0.005202257891,"remaining_time":0.2878582699},
{"learn":[22.10177672],"iteration":6,"passed_time":0.005828830165,"remaining_time":0.2756203978},
{"learn":[22.00827781],"iteration":7,"passed_time":0.006266134417,"remaining_time":0.2584780447},
{"learn":[21.8879153],"iteration":8,"passed_time":0.006684786783,"remaining_time":0.2443660946},
{"learn":[21.77915344],"iteration":9,"passed_time":0.00707286833,"remaining_time":0.2319900812},
{"learn":[21.68208858],"iteration":10,"passed_time":0.00745650067,"remaining_time":0.221661429},
{"learn":[21.58770379],"iteration":11,"passed_time":0.008846923598,"remaining_time":0.2403414244},
{"learn":[21.49101501],"iteration":12,"passed_time":0.009405632104,"remaining_time":0.2351408026},
{"learn":[21.40894799],"iteration":13,"passed_time":0.009902390308,"remaining_time":0.2291696043},
{"learn":[21.31034525],"iteration":14,"passed_time":0.01038840161,"remaining_time":0.2236969146},
{"learn":[21.24053315],"iteration":15,"passed_time":0.01084943935,"remaining_time":0.218344967},
{"learn":[21.1884252],"iteration":16,"passed_time":0.01140690587,"remaining_time":0.2153892226},
{"learn":[21.11628445],"iteration":17,"passed_time":0.01194795979,"remaining_time":0.212408174},
{"learn":[21.05137132],"iteration":18,"passed_time":0.0124129365,"remaining_time":0.2084066707},
{"learn":[20.98607817],"iteration":19,"passed_time":0.01296310376,"remaining_time":0.2061133498},
{"learn":[20.92119893],"iteration":20,"passed_time":0.013543705,"remaining_time":0.2044454517},
{"learn":[20.8209076],"iteration":21,"passed_time":0.01404569634,"remaining_time":0.2017472747},
{"learn":[20.77577424],"iteration":22,"passed_time":0.0145653585,"remaining_time":0.1994820838},
{"learn":[20.71303699],"iteration":23,"passed_time":0.01504383977,"remaining_time":0.1968235703},
{"learn":[20.65286172],"iteration":24,"passed_time":0.01552857004,"remaining_time":0.1944176969},
{"learn":[20.5720313],"iteration":25,"passed_time":0.01600428978,"remaining_time":0.1920514774},
{"learn":[20.486344],"iteration":26,"passed_time":0.01645181137,"remaining_time":0.1895004939},
{"learn":[20.41327157],"iteration":27,"passed_time":0.01686961487,"remaining_time":0.186770736},
{"learn":[20.3399392],"iteration":28,"passed_time":0.01732954476,"remaining_time":0.1846492872},
{"learn":[20.26644286],"iteration":29,"passed_time":0.01779938042,"remaining_time":0.1827403056},
{"learn":[20.19166332],"iteration":30,"passed_time":0.01829115133,"remaining_time":0.1811414019},
{"learn":[20.10730211],"iteration":31,"passed_time":0.02013059412,"remaining_time":0.1924988062},
{"learn":[20.03692841],"iteration":32,"passed_time":0.02083562747,"remaining_time":0.1925717084},
{"learn":[19.98503921],"iteration":33,"passed_time":0.02128784334,"remaining_time":0.190338364},
{"learn":[19.90645384],"iteration":34,"passed_time":0.021732356,"remaining_time":0.1881401105},
{"learn":[19.82446263],"iteration":35,"passed_time":0.0223134126,"remaining_time":0.187184739},
{"learn":[19.76739553],"iteration":36,"passed_time":0.02279428817,"remaining_time":0.1854346146},
{"learn":[19.71030814],"iteration":37,"passed_time":0.02328636679,"remaining_time":0.1838397378},
{"learn":[19.65883472],"iteration":38,"passed_time":0.02381799387,"remaining_time":0.1826046197},
{"learn":[19.60894093],"iteration":39,"passed_time":0.02438724828,"remaining_time":0.1816849997},
{"learn":[19.54461878],"iteration":40,"passed_time":0.02503883817,"remaining_time":0.1813789009},
{"learn":[19.49431936],"iteration":41,"passed_time":0.02568696766,"remaining_time":0.1810319625},
{"learn":[19.4565095],"iteration":42,"passed_time":0.02628653389,"remaining_time":0.1803378488},
{"learn":[19.39079518],"iteration":43,"passed_time":0.02683839541,"remaining_time":0.1793292784},
{"learn":[19.34211801],"iteration":44,"passed_time":0.0274766771,"remaining_time":0.1789036976},
{"learn":[19.27800836],"iteration":45,"passed_time":0.0281223945,"remaining_time":0.1785160695},
{"learn":[19.2367765],"iteration":46,"passed_time":0.02876873155,"remaining_time":0.1781212953},
{"learn":[19.19050666],"iteration":47,"passed_time":0.02920764153,"remaining_time":0.1764628343},
{"learn":[19.16504631],"iteration":48,"passed_time":0.02971288261,"remaining_time":0.1752453689},
{"learn":[19.09858418],"iteration":49,"passed_time":0.03013895677,"remaining_time":0.173600391},
{"learn":[19.0383305],"iteration":50,"passed_time":0.03056721185,"remaining_time":0.1720154863},
{"learn":[18.99658065],"iteration":51,"passed_time":0.03122630587,"remaining_time":0.1717446823},
{"learn":[18.96472302],"iteration":52,"passed_time":0.03175290933,"remaining_time":0.1707467766},
{"learn":[18.88910242],"iteration":53,"passed_time":0.03224075443,"remaining_time":0.1695624863},
{"learn":[18.84491235],"iteration":54,"passed_time":0.03274836661,"remaining_time":0.1685052319},
{"learn":[18.78782058],"iteration":55,"passed_time":0.03328911903,"remaining_time":0.1676344922},
{"learn":[18.74950573],"iteration":56,"passed_time":0.03375820864,"remaining_time":0.1664220461},
{"learn":[18.68726581],"iteration":57,"passed_time":0.03426355834,"remaining_time":0.1654102816},
{"learn":[18.65924247],"iteration":58,"passed_time":0.03479373277,"remaining_time":0.1645330753},
{"learn":[18.60662375],"iteration":59,"passed_time":0.03537096405,"remaining_time":0.1638854668},
{"learn":[18.56240041],"iteration":60,"passed_time":0.03678500258,"remaining_time":0.1670400937},
{"learn":[18.49841418],"iteration":61,"passed_time":0.03726322873,"remaining_time":0.1658814698},
{"learn":[18.46776677],"iteration":62,"passed_time":0.03780190458,"remaining_time":0.1650083137},
{"learn":[18.42032929],"iteration":63,"passed_time":0.0382512159,"remaining_time":0.1637630181},
{"learn":[18.36556165],"iteration":64,"passed_time":0.03872165574,"remaining_time":0.1626309541},
{"learn":[18.31134795],"iteration":65,"passed_time":0.03916892761,"remaining_time":0.1614234593},
{"learn":[18.27055254],"iteration":66,"passed_time":0.03960062609,"remaining_time":0.1601756667},
{"learn":[18.23604688],"iteration":67,"passed_time":0.0401069816,"remaining_time":0.1592483093},
{"learn":[18.19564288],"iteration":68,"passed_time":0.04060734633,"remaining_time":0.1583097995},
{"learn":[18.15833046],"iteration":69,"passed_time":0.04123523211,"remaining_time":0.1578720315},
{"learn":[18.12974793],"iteration":70,"passed_time":0.04174650919,"remaining_time":0.1569903937},
{"learn":[18.10009682],"iteration":71,"passed_time":0.04220738768,"remaining_time":0.1559328489},
{"learn":[18.07737895],"iteration":72,"passed_time":0.04261684398,"remaining_time":0.1547049816},
{"learn":[18.04871948],"iteration":73,"passed_time":0.04305897008,"remaining_time":0.1536157851},
{"learn":[17.99982834],"iteration":74,"passed_time":0.04349026191,"remaining_time":0.1525058517},
{"learn":[17.96840211],"iteration":75,"passed_time":0.04394878977,"remaining_time":0.15150767},
{"learn":[17.93087117],"iteration":76,"passed_time":0.04439020307,"remaining_time":0.1504654935},
{"learn":[17.908707],"iteration":77,"passed_time":0.04487923797,"remaining_time":0.1495974599},
{"learn":[17.87695402],"iteration":78,"passed_time":0.04534483393,"remaining_time":0.1486621771},
{"learn":[17.85023579],"iteration":79,"passed_time":0.04588860995,"remaining_time":0.1479907671},
{"learn":[17.82919942],"iteration":80,"passed_time":0.04635197474,"remaining_time":0.1470673766},
{"learn":[17.79442245],"iteration":81,"passed_time":0.04682752131,"remaining_time":0.1461932373},
{"learn":[17.75806215],"iteration":82,"passed_time":0.04727716158,"remaining_time":0.1452491109},
{"learn":[17.71935051],"iteration":83,"passed_time":0.04774597636,"remaining_time":0.144374738},
{"learn":[17.69551252],"iteration":84,"passed_time":0.04824316982,"remaining_time":0.1435943761},
{"learn":[17.67245834],"iteration":85,"passed_time":0.04868299053,"remaining_time":0.1426524839},
{"learn":[17.65789702],"iteration":86,"passed_time":0.04915395144,"remaining_time":0.1418119748},
{"learn":[17.62071453],"iteration":87,"passed_time":0.04963951743,"remaining_time":0.1410213563},
{"learn":[17.57896867],"iteration":88,"passed_time":0.05010682311,"remaining_time":0.1401865051},
{"learn":[17.54827239],"iteration":89,"passed_time":0.05056808544,"remaining_time":0.1393431688},
{"learn":[17.51957115],"iteration":90,"passed_time":0.05105218099,"remaining_time":0.1385702055},
{"learn":[17.48774608],"iteration":91,"passed_time":0.05158736242,"remaining_time":0.1379401213},
{"learn":[17.46792008],"iteration":92,"passed_time":0.05212482798,"remaining_time":0.1373180952},
{"learn":[17.45192224],"iteration":93,"passed_time":0.05261427418,"remaining_time":0.1365732223},
{"learn":[17.43488891],"iteration":94,"passed_time":0.05309613468,"remaining_time":0.1358143235},
{"learn":[17.40251379],"iteration":95,"passed_time":0.05355808237,"remaining_time":0.1350109993},
{"learn":[17.38055767],"iteration":96,"passed_time":0.05410819397,"remaining_time":0.1344337603},
{"learn":[17.34711493],"iteration":97,"passed_time":0.05454304208,"remaining_time":0.1335747969},
{"learn":[17.32091432],"iteration":98,"passed_time":0.05497394194,"remaining_time":0.1327148699},
{"learn":[17.29677259],"iteration":99,"passed_time":0.05550118901,"remaining_time":0.1320928299},
{"learn":[17.27307874],"iteration":100,"passed_time":0.05598859499,"remaining_time":0.1313791783},
{"learn":[17.25646811],"iteration":101,"passed_time":0.05664297231,"remaining_time":0.1310562889},
{"learn":[17.23567678],"iteration":102,"passed_time":0.05716056641,"remaining_time":0.1304148845},
{"learn":[17.22139429],"iteration":103,"passed_time":0.05794867086,"remaining_time":0.1303845094},
{"learn":[17.19746097],"iteration":104,"passed_time":0.05849038115,"remaining_time":0.129792941},
{"learn":[17.16872891],"iteration":105,"passed_time":0.05955872969,"remaining_time":0.1303549555},
{"learn":[17.13966866],"iteration":106,"passed_time":0.06003631223,"remaining_time":0.129611104},
{"learn":[17.11753449],"iteration":107,"passed_time":0.06053562399,"remaining_time":0.1289184585},
{"learn":[17.09590237],"iteration":108,"passed_time":0.06100824205,"remaining_time":0.1281732792},
{"learn":[17.07480181],"iteration":109,"passed_time":0.06153995109,"remaining_time":0.127555535},
{"learn":[17.05738127],"iteration":110,"passed_time":0.06208361733,"remaining_time":0.126963794},
{"learn":[17.03507019],"iteration":111,"passed_time":0.06254142118,"remaining_time":0.1261996535},
{"learn":[17.01558996],"iteration":112,"passed_time":0.06295081796,"remaining_time":0.125344549},
{"learn":[16.98508564],"iteration":113,"passed_time":0.06461097928,"remaining_time":0.1269549067},
{"learn":[16.97127125],"iteration":114,"passed_time":0.06813396512,"remaining_time":0.1321206454},
{"learn":[16.94810725],"iteration":115,"passed_time":0.06872596423,"remaining_time":0.1315272764},
{"learn":[16.92853312],"iteration":116,"passed_time":0.06926679704,"remaining_time":0.1308372833},
{"learn":[16.90254726],"iteration":117,"passed_time":0.06995238878,"remaining_time":0.1304197079},
{"learn":[16.88469378],"iteration":118,"passed_time":0.07056891999,"remaining_time":0.1298705334},
{"learn":[16.87072203],"iteration":119,"passed_time":0.07116070108,"remaining_time":0.1292752736},
{"learn":[16.84716501],"iteration":120,"passed_time":0.07196387408,"remaining_time":0.1290591791},
{"learn":[16.82771963],"iteration":121,"passed_time":0.07271209259,"remaining_time":0.1287361639},
{"learn":[16.8050763],"iteration":122,"passed_time":0.07318767782,"remaining_time":0.1279296807},
{"learn":[16.78685079],"iteration":123,"passed_time":0.07372808542,"remaining_time":0.1272404055},
{"learn":[16.77239707],"iteration":124,"passed_time":0.0746860329,"remaining_time":0.1272650001},
{"learn":[16.74705365],"iteration":125,"passed_time":0.07528941363,"remaining_time":0.1266774261},
{"learn":[16.73883188],"iteration":126,"passed_time":0.07577474691,"remaining_time":0.1258934772},
{"learn":[16.72175591],"iteration":127,"passed_time":0.07621897894,"remaining_time":0.1250467623},
{"learn":[16.69972093],"iteration":128,"passed_time":0.07665591365,"remaining_time":0.1241944648},
{"learn":[16.67237764],"iteration":129,"passed_time":0.07706483574,"remaining_time":0.1233037372},
{"learn":[16.65546848],"iteration":130,"passed_time":0.07757199334,"remaining_time":0.1225755925},
{"learn":[16.64434503],"iteration":131,"passed_time":0.07809110389,"remaining_time":0.12186945},
{"learn":[16.62263409],"iteration":132,"passed_time":0.07874594043,"remaining_time":0.1213753217},
{"learn":[16.60535286],"iteration":133,"passed_time":0.07917493151,"remaining_time":0.1205349704},
{"learn":[16.59631093],"iteration":134,"passed_time":0.07968635354,"remaining_time":0.119824665},
{"learn":[16.57976196],"iteration":135,"passed_time":0.08026934329,"remaining_time":0.1192235834},
{"learn":[16.56071482],"iteration":136,"passed_time":0.08097815595,"remaining_time":0.1188073675},
{"learn":[16.54404295],"iteration":137,"passed_time":0.08148860427,"remaining_time":0.1180994265},
{"learn":[16.53362896],"iteration":138,"passed_time":0.08196032707,"remaining_time":0.1173388855},
{"learn":[16.51655897],"iteration":139,"passed_time":0.08252207117,"remaining_time":0.1167097864},
{"learn":[16.50266489],"iteration":140,"passed_time":0.08309006387,"remaining_time":0.1160903729},
{"learn":[16.47964536],"iteration":141,"passed_time":0.08358056922,"remaining_time":0.1153647293},
{"learn":[16.46523158],"iteration":142,"passed_time":0.08409053782,"remaining_time":0.1146689152},
{"learn":[16.45029137],"iteration":143,"passed_time":0.08462967329,"remaining_time":0.1140149765},
{"learn":[16.42988106],"iteration":144,"passed_time":0.08525338122,"remaining_time":0.1134751902},
{"learn":[16.41202622],"iteration":145,"passed_time":0.08569745978,"remaining_time":0.1126980293},
{"learn":[16.39756015],"iteration":146,"passed_time":0.0861398441,"remaining_time":0.1119231988},
{"learn":[16.38344291],"iteration":147,"passed_time":0.08654491923,"remaining_time":0.1111049639},
{"learn":[16.36760885],"iteration":148,"passed_time":0.08701047112,"remaining_time":0.1103689869},
{"learn":[16.35098488],"iteration":149,"passed_time":0.08751068123,"remaining_time":0.1096800538},
{"learn":[16.34799984],"iteration":150,"passed_time":0.08793962592,"remaining_time":0.1089053645},
{"learn":[16.33462709],"iteration":151,"passed_time":0.08837461318,"remaining_time":0.1081426188},
{"learn":[16.32068485],"iteration":152,"passed_time":0.08883685001,"remaining_time":0.1074171062},
{"learn":[16.30618458],"iteration":153,"passed_time":0.08941305114,"remaining_time":0.106831178},
{"learn":[16.29044882],"iteration":154,"passed_time":0.08994066814,"remaining_time":0.1061880146},
{"learn":[16.28104842],"iteration":155,"passed_time":0.09044942761,"remaining_time":0.1055243322},
{"learn":[16.26304502],"iteration":156,"passed_time":0.09102288035,"remaining_time":0.104937206},
{"learn":[16.24745918],"iteration":157,"passed_time":0.09142054683,"remaining_time":0.1041499901},
{"learn":[16.2309831],"iteration":158,"passed_time":0.0919514731,"remaining_time":0.1035176961},
{"learn":[16.22858712],"iteration":159,"passed_time":0.09237527047,"remaining_time":0.1027674884},
{"learn":[16.21606988],"iteration":160,"passed_time":0.09285567677,"remaining_time":0.1020835701},
{"learn":[16.20142297],"iteration":161,"passed_time":0.09335100551,"remaining_time":0.1014183764},
{"learn":[16.18597306],"iteration":162,"passed_time":0.09386758144,"remaining_time":0.1007780782},
{"learn":[16.17027527],"iteration":163,"passed_time":0.09445106751,"remaining_time":0.1002102789},
{"learn":[16.15692885],"iteration":164,"passed_time":0.09506787395,"remaining_time":0.09967722541},
{"learn":[16.13991977],"iteration":165,"passed_time":0.09568871559,"remaining_time":0.09914734387},
{"learn":[16.12788629],"iteration":166,"passed_time":0.09610639772,"remaining_time":0.09840834736},
{"learn":[16.11130744],"iteration":167,"passed_time":0.0966172349,"remaining_time":0.09776744008},
{"learn":[16.09664531],"iteration":168,"passed_time":0.09710539311,"remaining_time":0.09710539311},
{"learn":[16.08361703],"iteration":169,"passed_time":0.09753632699,"remaining_time":0.09638884079},
{"learn":[16.07239464],"iteration":170,"passed_time":0.09805344989,"remaining_time":0.09575980194},
{"learn":[16.05786586],"iteration":171,"passed_time":0.0985541285,"remaining_time":0.09511619378},
{"learn":[16.04333996],"iteration":172,"passed_time":0.09897982229,"remaining_time":0.09440272068},
{"learn":[16.0292017],"iteration":173,"passed_time":0.09955864849,"remaining_time":0.09383688708},
{"learn":[16.01540527],"iteration":174,"passed_time":0.1000906892,"remaining_time":0.09322732764},
{"learn":[16.00202064],"iteration":175,"passed_time":0.1005489654,"remaining_time":0.09255075225},
{"learn":[15.98625344],"iteration":176,"passed_time":0.1009488778,"remaining_time":0.09182355547},
{"learn":[15.97216232],"iteration":177,"passed_time":0.1015055951,"remaining_time":0.09124098439},
{"learn":[15.96160283],"iteration":178,"passed_time":0.1021074911,"remaining_time":0.09069883289},
{"learn":[15.94752053],"iteration":179,"passed_time":0.1026445929,"remaining_time":0.09009914268},
{"learn":[15.94716544],"iteration":180,"passed_time":0.1031150695,"remaining_time":0.0894423531},
{"learn":[15.94400209],"iteration":181,"passed_time":0.1035346913,"remaining_time":0.08874402114},
{"learn":[15.94270422],"iteration":182,"passed_time":0.1039741974,"remaining_time":0.08806557702},
{"learn":[15.93235761],"iteration":183,"passed_time":0.1045765313,"remaining_time":0.08752600991},
{"learn":[15.91961253],"iteration":184,"passed_time":0.1050871544,"remaining_time":0.08690991685},
{"learn":[15.90709724],"iteration":185,"passed_time":0.1055843811,"remaining_time":0.08628401033},
{"learn":[15.89941905],"iteration":186,"passed_time":0.1066047334,"remaining_time":0.08608189705},
{"learn":[15.88720731],"iteration":187,"passed_time":0.1070620491,"remaining_time":0.08542184766},
{"learn":[15.87715071],"iteration":188,"passed_time":0.1075397584,"remaining_time":0.08478002117},
{"learn":[15.86527816],"iteration":189,"passed_time":0.1079913867,"remaining_time":0.08411960649},
{"learn":[15.85638056],"iteration":190,"passed_time":0.1083921471,"remaining_time":0.08342222843},
{"learn":[15.84520276],"iteration":191,"passed_time":0.1088177744,"remaining_time":0.08274684932},
{"learn":[15.83563554],"iteration":192,"passed_time":0.1092724758,"remaining_time":0.08209590154},
{"learn":[15.82425455],"iteration":193,"passed_time":0.1097956347,"remaining_time":0.08149779074},
{"learn":[15.81519642],"iteration":194,"passed_time":0.1102182261,"remaining_time":0.08082669912},
{"learn":[15.80419142],"iteration":195,"passed_time":0.1106893814,"remaining_time":0.08019332736},
{"learn":[15.79353906],"iteration":196,"passed_time":0.1111999979,"remaining_time":0.0795898462},
{"learn":[15.78131232],"iteration":197,"passed_time":0.111705774,"remaining_time":0.07898388057},
{"learn":[15.77841406],"iteration":198,"passed_time":0.1121743506,"remaining_time":0.07835293837},
{"learn":[15.77554588],"iteration":199,"passed_time":0.1125974615,"remaining_time":0.07769224841},
{"learn":[15.76648752],"iteration":200,"passed_time":0.1130959464,"remaining_time":0.0770852968},
{"learn":[15.75572559],"iteration":201,"passed_time":0.1135670391,"remaining_time":0.07646097683},
{"learn":[15.75535958],"iteration":202,"passed_time":0.1139733868,"remaining_time":0.07579510943},
{"learn":[15.74478907],"iteration":203,"passed_time":0.1144068997,"remaining_time":0.07514963021},
{"learn":[15.74444293],"iteration":204,"passed_time":0.1148630399,"remaining_time":0.07452089904},
{"learn":[15.73561079],"iteration":205,"passed_time":0.1153291314,"remaining_time":0.07390022011},
{"learn":[15.7352637],"iteration":206,"passed_time":0.1157983261,"remaining_time":0.07328299867},
{"learn":[15.72851918],"iteration":207,"passed_time":0.1163213443,"remaining_time":0.07270084022},
{"learn":[15.71849949],"iteration":208,"passed_time":0.1167714736,"remaining_time":0.07207425883},
{"learn":[15.70850606],"iteration":209,"passed_time":0.1171683716,"remaining_time":0.07141691223},
{"learn":[15.70817002],"iteration":210,"passed_time":0.1176811091,"remaining_time":0.07083175761},
{"learn":[15.69835457],"iteration":211,"passed_time":0.1181637458,"remaining_time":0.07022939609},
{"learn":[15.69726829],"iteration":212,"passed_time":0.1186312065,"remaining_time":0.06961925263},
{"learn":[15.69126651],"iteration":213,"passed_time":0.1193362742,"remaining_time":0.06914812152},
{"learn":[15.68168939],"iteration":214,"passed_time":0.1198699102,"remaining_time":0.06857673935},
{"learn":[15.6733662],"iteration":215,"passed_time":0.120501084,"remaining_time":0.06806079746},
{"learn":[15.67071771],"iteration":216,"passed_time":0.1210523676,"remaining_time":0.06749924648},
{"learn":[15.66140659],"iteration":217,"passed_time":0.1216088879,"remaining_time":0.06694067223},
{"learn":[15.65193439],"iteration":218,"passed_time":0.122079175,"remaining_time":0.06633525949},
{"learn":[15.64622809],"iteration":219,"passed_time":0.1225423094,"remaining_time":0.0657272387},
{"learn":[15.63703485],"iteration":220,"passed_time":0.1230348933,"remaining_time":0.06513611997},
{"learn":[15.62802347],"iteration":221,"passed_time":0.1235248814,"remaining_time":0.06454453263},
{"learn":[15.62005566],"iteration":222,"passed_time":0.1335875399,"remaining_time":0.06889043536},
{"learn":[15.6121973],"iteration":223,"passed_time":0.1342641922,"remaining_time":0.06833088354},
{"learn":[15.60601645],"iteration":224,"passed_time":0.1348075021,"remaining_time":0.06770332325},
{"learn":[15.59925629],"iteration":225,"passed_time":0.1355298494,"remaining_time":0.06716523512},
{"learn":[15.59346933],"iteration":226,"passed_time":0.1359992019,"remaining_time":0.06650181239},
{"learn":[15.58488569],"iteration":227,"passed_time":0.1364827602,"remaining_time":0.06584694569},
{"learn":[15.58314196],"iteration":228,"passed_time":0.1369301186,"remaining_time":0.06517634467},
{"learn":[15.57575299],"iteration":229,"passed_time":0.1374929014,"remaining_time":0.06456188412},
{"learn":[15.57541581],"iteration":230,"passed_time":0.1386322544,"remaining_time":0.06421494036},
{"learn":[15.56703691],"iteration":231,"passed_time":0.1391950557,"remaining_time":0.06359774098},
{"learn":[15.56662703],"iteration":232,"passed_time":0.1397435825,"remaining_time":0.06297457579},
{"learn":[15.5584365],"iteration":233,"passed_time":0.140202788,"remaining_time":0.0623123502},
{"learn":[15.54854734],"iteration":234,"passed_time":0.1408623408,"remaining_time":0.06173966427},
{"learn":[15.5412056],"iteration":235,"passed_time":0.1412916543,"remaining_time":0.06106673193},
{"learn":[15.53336515],"iteration":236,"passed_time":0.1419389496,"remaining_time":0.06048875066},
{"learn":[15.52537721],"iteration":237,"passed_time":0.142393201,"remaining_time":0.05982907606},
{"learn":[15.51823908],"iteration":238,"passed_time":0.1428996655,"remaining_time":0.05919274848},
{"learn":[15.5179174],"iteration":239,"passed_time":0.144174576,"remaining_time":0.05887128521},
{"learn":[15.51098691],"iteration":240,"passed_time":0.1447743085,"remaining_time":0.05827015735},
{"learn":[15.50402324],"iteration":241,"passed_time":0.1453726888,"remaining_time":0.05766850464},
{"learn":[15.50256475],"iteration":242,"passed_time":0.1474654427,"remaining_time":0.05765109899},
{"learn":[15.50085988],"iteration":243,"passed_time":0.1479324998,"remaining_time":0.05699038927},
{"learn":[15.49572926],"iteration":244,"passed_time":0.148708935,"remaining_time":0.05644869779},
{"learn":[15.48831934],"iteration":245,"passed_time":0.1492256609,"remaining_time":0.05580797076},
{"learn":[15.48072297],"iteration":246,"passed_time":0.1500255977,"remaining_time":0.05527258864},
{"learn":[15.47160981],"iteration":247,"passed_time":0.1506014568,"remaining_time":0.05465375447},
{"learn":[15.47015882],"iteration":248,"passed_time":0.1512127139,"remaining_time":0.0540479178},
{"learn":[15.46852768],"iteration":249,"passed_time":0.1520202665,"remaining_time":0.0535111338},
{"learn":[15.46621868],"iteration":250,"passed_time":0.1527536263,"remaining_time":0.05294647606},
{"learn":[15.45853562],"iteration":251,"passed_time":0.1532336716,"remaining_time":0.05229403078},
{"learn":[15.45154397],"iteration":252,"passed_time":0.1536975667,"remaining_time":0.05163752242},
{"learn":[15.44311811],"iteration":253,"passed_time":0.1543814174,"remaining_time":0.05105527191},
{"learn":[15.44216034],"iteration":254,"passed_time":0.1549693601,"remaining_time":0.0504410074},
{"learn":[15.43752228],"iteration":255,"passed_time":0.1554680317,"remaining_time":0.04979835391},
{"learn":[15.43082222],"iteration":256,"passed_time":0.1560453132,"remaining_time":0.04918159678},
{"learn":[15.42568857],"iteration":257,"passed_time":0.1577043528,"remaining_time":0.04890057451},
{"learn":[15.4191554],"iteration":258,"passed_time":0.1583111696,"remaining_time":0.04828796292},
{"learn":[15.41273524],"iteration":259,"passed_time":0.1588731128,"remaining_time":0.04766193383},
{"learn":[15.41129408],"iteration":260,"passed_time":0.1593544893,"remaining_time":0.04701262711},
{"learn":[15.40499359],"iteration":261,"passed_time":0.1598076623,"remaining_time":0.04635642112},
{"learn":[15.39732555],"iteration":262,"passed_time":0.1602444741,"remaining_time":0.04569709336},
{"learn":[15.39291295],"iteration":263,"passed_time":0.1606604345,"remaining_time":0.04503360663},
{"learn":[15.38600391],"iteration":264,"passed_time":0.1611066982,"remaining_time":0.04438033574},
{"learn":[15.38004674],"iteration":265,"passed_time":0.161531247,"remaining_time":0.04372274356},
{"learn":[15.37419043],"iteration":266,"passed_time":0.1620128431,"remaining_time":0.0430820669},
{"learn":[15.368433],"iteration":267,"passed_time":0.1625396329,"remaining_time":0.04245438173},
{"learn":[15.36106699],"iteration":268,"passed_time":0.1630558714,"remaining_time":0.04182474025},
{"learn":[15.36071338],"iteration":269,"passed_time":0.163543059,"remaining_time":0.04118862226},
{"learn":[15.35860281],"iteration":270,"passed_time":0.1640277483,"remaining_time":0.04055298573},
{"learn":[15.35209843],"iteration":271,"passed_time":0.1645400953,"remaining_time":0.03992517019},
{"learn":[15.34529265],"iteration":272,"passed_time":0.1649956502,"remaining_time":0.03928467862},
{"learn":[15.34176242],"iteration":273,"passed_time":0.1655187612,"remaining_time":0.03866131648},
{"learn":[15.33760183],"iteration":274,"passed_time":0.1660412641,"remaining_time":0.03803854414},
{"learn":[15.33298961],"iteration":275,"passed_time":0.1665412767,"remaining_time":0.03741144621},
{"learn":[15.32864949],"iteration":276,"passed_time":0.1672142251,"remaining_time":0.03682334921},
{"learn":[15.32263358],"iteration":277,"passed_time":0.1677263526,"remaining_time":0.03619993222},
{"learn":[15.31836356],"iteration":278,"passed_time":0.1683956087,"remaining_time":0.0356105409},
{"learn":[15.3131689],"iteration":279,"passed_time":0.169001635,"remaining_time":0.03500748153},
{"learn":[15.31161799],"iteration":280,"passed_time":0.1695278055,"remaining_time":0.03438820254},
{"learn":[15.30957949],"iteration":281,"passed_time":0.1702780352,"remaining_time":0.03381407792},
{"learn":[15.30346892],"iteration":282,"passed_time":0.1708695117,"remaining_time":0.03320785563},
{"learn":[15.30314843],"iteration":283,"passed_time":0.1713925222,"remaining_time":0.03258871901},
{"learn":[15.30113188],"iteration":284,"passed_time":0.1780185779,"remaining_time":0.03310520922},
{"learn":[15.29890173],"iteration":285,"passed_time":0.178789981,"remaining_time":0.03250726927},
{"learn":[15.29859108],"iteration":286,"passed_time":0.1804270184,"remaining_time":0.03206194403},
{"learn":[15.29826912],"iteration":287,"passed_time":0.1811791921,"remaining_time":0.03145472085},
{"learn":[15.29381744],"iteration":288,"passed_time":0.1816850876,"remaining_time":0.03080473804},
{"learn":[15.29254256],"iteration":289,"passed_time":0.1821317518,"remaining_time":0.03014594513},
{"learn":[15.28860512],"iteration":290,"passed_time":0.1826221679,"remaining_time":0.0294956766},
{"learn":[15.28828095],"iteration":291,"passed_time":0.183144687,"remaining_time":0.02885156028},
{"learn":[15.28203022],"iteration":292,"passed_time":0.1836329028,"remaining_time":0.02820300555},
{"learn":[15.28122099],"iteration":293,"passed_time":0.1841257321,"remaining_time":0.02755623202},
{"learn":[15.27635588],"iteration":294,"passed_time":0.1846508698,"remaining_time":0.02691521153},
{"learn":[15.27599698],"iteration":295,"passed_time":0.1851390442,"remaining_time":0.02626972925},
{"learn":[15.2738452],"iteration":296,"passed_time":0.1855594778,"remaining_time":0.02561595485},
{"learn":[15.26822662],"iteration":297,"passed_time":0.1860428849,"remaining_time":0.02497219932},
{"learn":[15.26318773],"iteration":298,"passed_time":0.1864846214,"remaining_time":0.02432408105},
{"learn":[15.2616887],"iteration":299,"passed_time":0.1868955164,"remaining_time":0.02367343208},
{"learn":[15.25956737],"iteration":300,"passed_time":0.1873429298,"remaining_time":0.02302886512},
{"learn":[15.2557434],"iteration":301,"passed_time":0.1878230384,"remaining_time":0.02238950127},
{"learn":[15.25381907],"iteration":302,"passed_time":0.1882997995,"remaining_time":0.02175080193},
{"learn":[15.24835557],"iteration":303,"passed_time":0.1887761153,"remaining_time":0.02111311816},
{"learn":[15.24688058],"iteration":304,"passed_time":0.1892346683,"remaining_time":0.02047457067},
{"learn":[15.24311461],"iteration":305,"passed_time":0.1898969962,"remaining_time":0.01985850941},
{"learn":[15.24104758],"iteration":306,"passed_time":0.19046803,"remaining_time":0.01923292811},
{"learn":[15.23706563],"iteration":307,"passed_time":0.1909166663,"remaining_time":0.01859577919},
{"learn":[15.23382403],"iteration":308,"passed_time":0.191387606,"remaining_time":0.01796194361},
{"learn":[15.23255267],"iteration":309,"passed_time":0.1918572766,"remaining_time":0.01732904434},
{"learn":[15.22686973],"iteration":310,"passed_time":0.1922995569,"remaining_time":0.01669481684},
{"learn":[15.22542037],"iteration":311,"passed_time":0.1927514906,"remaining_time":0.01606262422},
{"learn":[15.22174714],"iteration":312,"passed_time":0.1932521051,"remaining_time":0.01543547165},
{"learn":[15.21734627],"iteration":313,"passed_time":0.1939014329,"remaining_time":0.01482049168},
{"learn":[15.2122463],"iteration":314,"passed_time":0.1944562484,"remaining_time":0.01419839274},
{"learn":[15.21081125],"iteration":315,"passed_time":0.1950280954,"remaining_time":0.01357790538},
{"learn":[15.20545774],"iteration":316,"passed_time":0.1955921959,"remaining_time":0.01295721172},
{"learn":[15.2018643],"iteration":317,"passed_time":0.1961372549,"remaining_time":0.01233567641},
{"learn":[15.20077412],"iteration":318,"passed_time":0.196625386,"remaining_time":0.01171122989},
{"learn":[15.19721463],"iteration":319,"passed_time":0.1970567158,"remaining_time":0.01108444026},
{"learn":[15.19690628],"iteration":320,"passed_time":0.1975662197,"remaining_time":0.01046300852},
{"learn":[15.19354536],"iteration":321,"passed_time":0.1981136061,"remaining_time":0.009844154342},
{"learn":[15.19159628],"iteration":322,"passed_time":0.1986454864,"remaining_time":0.009225022589},
{"learn":[15.18965753],"iteration":323,"passed_time":0.1992748059,"remaining_time":0.008610639762},
{"learn":[15.18937071],"iteration":324,"passed_time":0.1998719489,"remaining_time":0.007994877954},
{"learn":[15.18527954],"iteration":325,"passed_time":0.2003677024,"remaining_time":0.007375498248},
{"learn":[15.18453838],"iteration":326,"passed_time":0.2008567563,"remaining_time":0.006756649293},
{"learn":[15.18368197],"iteration":327,"passed_time":0.2013675521,"remaining_time":0.006139254637},
{"learn":[15.1833341],"iteration":328,"passed_time":0.2018700715,"remaining_time":0.00552228159},
{"learn":[15.17952525],"iteration":329,"passed_time":0.2023603565,"remaining_time":0.004905705611},
{"learn":[15.17805618],"iteration":330,"passed_time":0.202876973,"remaining_time":0.00429044958},
{"learn":[15.17406433],"iteration":331,"passed_time":0.2033660272,"remaining_time":0.003675289648},
{"learn":[15.17030763],"iteration":332,"passed_time":0.2038237197,"remaining_time":0.003060416212},
{"learn":[15.16070908],"iteration":333,"passed_time":0.2044446259,"remaining_time":0.002448438634},
{"learn":[15.15883247],"iteration":334,"passed_time":0.2050359919,"remaining_time":0.001836143211},
{"learn":[15.15744176],"iteration":335,"passed_time":0.2055161639,"remaining_time":0.0012233105},
{"learn":[15.15557917],"iteration":336,"passed_time":0.2060791543,"remaining_time":0.0006115108435},
{"learn":[15.15382869],"iteration":337,"passed_time":0.2066165359,"remaining_time":0}
]}
</file>

<file path="catboost_info/learn_error.tsv">
iter	RMSE
0	22.62912629
1	22.55904783
2	22.45396966
3	22.34197654
4	22.26364277
5	22.18266737
6	22.10177672
7	22.00827781
8	21.8879153
9	21.77915344
10	21.68208858
11	21.58770379
12	21.49101501
13	21.40894799
14	21.31034525
15	21.24053315
16	21.1884252
17	21.11628445
18	21.05137132
19	20.98607817
20	20.92119893
21	20.8209076
22	20.77577424
23	20.71303699
24	20.65286172
25	20.5720313
26	20.486344
27	20.41327157
28	20.3399392
29	20.26644286
30	20.19166332
31	20.10730211
32	20.03692841
33	19.98503921
34	19.90645384
35	19.82446263
36	19.76739553
37	19.71030814
38	19.65883472
39	19.60894093
40	19.54461878
41	19.49431936
42	19.4565095
43	19.39079518
44	19.34211801
45	19.27800836
46	19.2367765
47	19.19050666
48	19.16504631
49	19.09858418
50	19.0383305
51	18.99658065
52	18.96472302
53	18.88910242
54	18.84491235
55	18.78782058
56	18.74950573
57	18.68726581
58	18.65924247
59	18.60662375
60	18.56240041
61	18.49841418
62	18.46776677
63	18.42032929
64	18.36556165
65	18.31134795
66	18.27055254
67	18.23604688
68	18.19564288
69	18.15833046
70	18.12974793
71	18.10009682
72	18.07737895
73	18.04871948
74	17.99982834
75	17.96840211
76	17.93087117
77	17.908707
78	17.87695402
79	17.85023579
80	17.82919942
81	17.79442245
82	17.75806215
83	17.71935051
84	17.69551252
85	17.67245834
86	17.65789702
87	17.62071453
88	17.57896867
89	17.54827239
90	17.51957115
91	17.48774608
92	17.46792008
93	17.45192224
94	17.43488891
95	17.40251379
96	17.38055767
97	17.34711493
98	17.32091432
99	17.29677259
100	17.27307874
101	17.25646811
102	17.23567678
103	17.22139429
104	17.19746097
105	17.16872891
106	17.13966866
107	17.11753449
108	17.09590237
109	17.07480181
110	17.05738127
111	17.03507019
112	17.01558996
113	16.98508564
114	16.97127125
115	16.94810725
116	16.92853312
117	16.90254726
118	16.88469378
119	16.87072203
120	16.84716501
121	16.82771963
122	16.8050763
123	16.78685079
124	16.77239707
125	16.74705365
126	16.73883188
127	16.72175591
128	16.69972093
129	16.67237764
130	16.65546848
131	16.64434503
132	16.62263409
133	16.60535286
134	16.59631093
135	16.57976196
136	16.56071482
137	16.54404295
138	16.53362896
139	16.51655897
140	16.50266489
141	16.47964536
142	16.46523158
143	16.45029137
144	16.42988106
145	16.41202622
146	16.39756015
147	16.38344291
148	16.36760885
149	16.35098488
150	16.34799984
151	16.33462709
152	16.32068485
153	16.30618458
154	16.29044882
155	16.28104842
156	16.26304502
157	16.24745918
158	16.2309831
159	16.22858712
160	16.21606988
161	16.20142297
162	16.18597306
163	16.17027527
164	16.15692885
165	16.13991977
166	16.12788629
167	16.11130744
168	16.09664531
169	16.08361703
170	16.07239464
171	16.05786586
172	16.04333996
173	16.0292017
174	16.01540527
175	16.00202064
176	15.98625344
177	15.97216232
178	15.96160283
179	15.94752053
180	15.94716544
181	15.94400209
182	15.94270422
183	15.93235761
184	15.91961253
185	15.90709724
186	15.89941905
187	15.88720731
188	15.87715071
189	15.86527816
190	15.85638056
191	15.84520276
192	15.83563554
193	15.82425455
194	15.81519642
195	15.80419142
196	15.79353906
197	15.78131232
198	15.77841406
199	15.77554588
200	15.76648752
201	15.75572559
202	15.75535958
203	15.74478907
204	15.74444293
205	15.73561079
206	15.7352637
207	15.72851918
208	15.71849949
209	15.70850606
210	15.70817002
211	15.69835457
212	15.69726829
213	15.69126651
214	15.68168939
215	15.6733662
216	15.67071771
217	15.66140659
218	15.65193439
219	15.64622809
220	15.63703485
221	15.62802347
222	15.62005566
223	15.6121973
224	15.60601645
225	15.59925629
226	15.59346933
227	15.58488569
228	15.58314196
229	15.57575299
230	15.57541581
231	15.56703691
232	15.56662703
233	15.5584365
234	15.54854734
235	15.5412056
236	15.53336515
237	15.52537721
238	15.51823908
239	15.5179174
240	15.51098691
241	15.50402324
242	15.50256475
243	15.50085988
244	15.49572926
245	15.48831934
246	15.48072297
247	15.47160981
248	15.47015882
249	15.46852768
250	15.46621868
251	15.45853562
252	15.45154397
253	15.44311811
254	15.44216034
255	15.43752228
256	15.43082222
257	15.42568857
258	15.4191554
259	15.41273524
260	15.41129408
261	15.40499359
262	15.39732555
263	15.39291295
264	15.38600391
265	15.38004674
266	15.37419043
267	15.368433
268	15.36106699
269	15.36071338
270	15.35860281
271	15.35209843
272	15.34529265
273	15.34176242
274	15.33760183
275	15.33298961
276	15.32864949
277	15.32263358
278	15.31836356
279	15.3131689
280	15.31161799
281	15.30957949
282	15.30346892
283	15.30314843
284	15.30113188
285	15.29890173
286	15.29859108
287	15.29826912
288	15.29381744
289	15.29254256
290	15.28860512
291	15.28828095
292	15.28203022
293	15.28122099
294	15.27635588
295	15.27599698
296	15.2738452
297	15.26822662
298	15.26318773
299	15.2616887
300	15.25956737
301	15.2557434
302	15.25381907
303	15.24835557
304	15.24688058
305	15.24311461
306	15.24104758
307	15.23706563
308	15.23382403
309	15.23255267
310	15.22686973
311	15.22542037
312	15.22174714
313	15.21734627
314	15.2122463
315	15.21081125
316	15.20545774
317	15.2018643
318	15.20077412
319	15.19721463
320	15.19690628
321	15.19354536
322	15.19159628
323	15.18965753
324	15.18937071
325	15.18527954
326	15.18453838
327	15.18368197
328	15.1833341
329	15.17952525
330	15.17805618
331	15.17406433
332	15.17030763
333	15.16070908
334	15.15883247
335	15.15744176
336	15.15557917
337	15.15382869
</file>

<file path="catboost_info/test_error.tsv">
iter	RMSE
0	24.97325187
1	24.31553624
2	23.77146897
3	23.2779618
4	22.73472951
5	22.24506633
6	21.81780246
7	21.36353132
8	20.95336663
9	20.65617255
10	20.29695762
11	20.0518295
12	19.76617563
13	19.5329565
14	19.2730599
15	19.14734089
16	18.94805374
17	18.71461401
18	18.59937216
19	18.35121254
20	18.08542362
21	17.96993205
22	17.83179983
23	17.73109078
24	17.61062296
25	17.47168209
26	17.23803424
27	17.0635614
28	16.87146165
29	16.85122515
30	16.75733739
31	16.61696489
32	16.55195885
33	16.4221066
34	16.38678574
35	16.36416354
36	16.26262553
37	16.25978103
38	16.20987503
39	16.10533122
40	16.02941966
41	15.95462384
42	15.90093868
43	15.82610209
44	15.81282117
45	15.80384161
46	15.73022655
47	15.7192581
48	15.67188292
49	15.59855473
50	15.59164642
51	15.54041757
52	15.4970452
53	15.47598982
54	15.44507351
55	15.43404264
56	15.41071774
57	15.40256238
58	15.36860136
59	15.35548265
60	15.27341602
61	15.25980229
62	15.22161108
63	15.23377005
64	15.21418565
65	15.20968828
66	15.1576913
67	15.14753424
68	15.08621542
69	15.04175651
70	15.02460658
71	15.02906852
72	15.03849461
73	15.0349015
74	15.02265994
75	15.01408698
76	14.98521813
77	15.01310161
78	15.00451967
79	14.9895131
80	15.03652215
81	15.0205681
82	14.97991059
83	14.98140968
84	15.0080292
85	14.99690461
86	14.97389321
87	14.97452306
88	14.97619232
89	14.99684783
90	14.98302452
91	14.97038131
92	14.91241705
93	14.89556566
94	14.88389895
95	14.87288461
96	14.89726914
97	14.88653216
98	14.86894885
99	14.85936989
100	14.84912152
101	14.80493053
102	14.7895744
103	14.78035929
104	14.7716599
105	14.76569241
106	14.75716362
107	14.76410341
108	14.75570419
109	14.74993443
110	14.72471755
111	14.71677698
112	14.69647445
113	14.68888779
114	14.68172779
115	14.67122828
116	14.6227691
117	14.61269461
118	14.60088434
119	14.594794
120	14.55987272
121	14.52710618
122	14.51346844
123	14.48265421
124	14.47773126
125	14.48888483
126	14.49598621
127	14.50479939
128	14.49086444
129	14.4714125
130	14.49675416
131	14.50032204
132	14.48770804
133	14.48409605
134	14.49077726
135	14.49924156
136	14.51024535
137	14.46946531
138	14.46433295
139	14.45415801
140	14.47658827
141	14.46442625
142	14.47908826
143	14.48711329
144	14.48052449
145	14.48519585
146	14.49879593
147	14.49275231
148	14.50231444
149	14.47749779
150	14.49417515
151	14.50579119
152	14.47472096
153	14.46391714
154	14.45678045
155	14.4414752
156	14.4171507
157	14.4053544
158	14.41270764
159	14.42029168
160	14.43195536
161	14.42798028
162	14.4183167
163	14.40756631
164	14.41582824
165	14.41754941
166	14.39035787
167	14.38068048
168	14.38849253
169	14.39644093
170	14.3936455
171	14.39103717
172	14.39866756
173	14.39897722
174	14.41531284
175	14.40847843
176	14.39878786
177	14.40486187
178	14.39767118
179	14.38350506
180	14.39071316
181	14.37933045
182	14.36202221
183	14.36278248
184	14.35936964
185	14.37395729
186	14.38034167
187	14.37377445
188	14.38218024
189	14.38700128
190	14.39172304
191	14.39066588
192	14.38724058
193	14.39364186
194	14.40321268
195	14.40126215
196	14.40740281
197	14.41590147
198	14.41197433
199	14.40420442
200	14.41364655
201	14.42075969
202	14.42969268
203	14.42958595
204	14.43329141
205	14.42660143
206	14.43029132
207	14.42763134
208	14.44087646
209	14.4462925
210	14.45155837
211	14.456682
212	14.46027799
213	14.45621015
214	14.46241121
215	14.47046578
216	14.46843565
217	14.47329272
218	14.49021497
219	14.48885211
220	14.50160408
221	14.49796165
222	14.49707619
223	14.5098219
224	14.50736057
225	14.51349183
226	14.50968887
227	14.51479045
228	14.51969138
229	14.51215244
230	14.50896371
231	14.52041472
232	14.51285299
233	14.51698549
234	14.5415415
</file>

<file path="catboost_info/time_left.tsv">
iter	Passed	Remaining
0	0	198
1	1	185
2	1	188
3	2	191
4	2	191
5	5	287
6	5	275
7	6	258
8	6	244
9	7	231
10	7	221
11	8	240
12	9	235
13	9	229
14	10	223
15	10	218
16	11	215
17	11	212
18	12	208
19	12	206
20	13	204
21	14	201
22	14	199
23	15	196
24	15	194
25	16	192
26	16	189
27	16	186
28	17	184
29	17	182
30	18	181
31	20	192
32	20	192
33	21	190
34	21	188
35	22	187
36	22	185
37	23	183
38	23	182
39	24	181
40	25	181
41	25	181
42	26	180
43	26	179
44	27	178
45	28	178
46	28	178
47	29	176
48	29	175
49	30	173
50	30	172
51	31	171
52	31	170
53	32	169
54	32	168
55	33	167
56	33	166
57	34	165
58	34	164
59	35	163
60	36	167
61	37	165
62	37	165
63	38	163
64	38	162
65	39	161
66	39	160
67	40	159
68	40	158
69	41	157
70	41	156
71	42	155
72	42	154
73	43	153
74	43	152
75	43	151
76	44	150
77	44	149
78	45	148
79	45	147
80	46	147
81	46	146
82	47	145
83	47	144
84	48	143
85	48	142
86	49	141
87	49	141
88	50	140
89	50	139
90	51	138
91	51	137
92	52	137
93	52	136
94	53	135
95	53	135
96	54	134
97	54	133
98	54	132
99	55	132
100	55	131
101	56	131
102	57	130
103	57	130
104	58	129
105	59	130
106	60	129
107	60	128
108	61	128
109	61	127
110	62	126
111	62	126
112	62	125
113	64	126
114	68	132
115	68	131
116	69	130
117	69	130
118	70	129
119	71	129
120	71	129
121	72	128
122	73	127
123	73	127
124	74	127
125	75	126
126	75	125
127	76	125
128	76	124
129	77	123
130	77	122
131	78	121
132	78	121
133	79	120
134	79	119
135	80	119
136	80	118
137	81	118
138	81	117
139	82	116
140	83	116
141	83	115
142	84	114
143	84	114
144	85	113
145	85	112
146	86	111
147	86	111
148	87	110
149	87	109
150	87	108
151	88	108
152	88	107
153	89	106
154	89	106
155	90	105
156	91	104
157	91	104
158	91	103
159	92	102
160	92	102
161	93	101
162	93	100
163	94	100
164	95	99
165	95	99
166	96	98
167	96	97
168	97	97
169	97	96
170	98	95
171	98	95
172	98	94
173	99	93
174	100	93
175	100	92
176	100	91
177	101	91
178	102	90
179	102	90
180	103	89
181	103	88
182	103	88
183	104	87
184	105	86
185	105	86
186	106	86
187	107	85
188	107	84
189	107	84
190	108	83
191	108	82
192	109	82
193	109	81
194	110	80
195	110	80
196	111	79
197	111	78
198	112	78
199	112	77
200	113	77
201	113	76
202	113	75
203	114	75
204	114	74
205	115	73
206	115	73
207	116	72
208	116	72
209	117	71
210	117	70
211	118	70
212	118	69
213	119	69
214	119	68
215	120	68
216	121	67
217	121	66
218	122	66
219	122	65
220	123	65
221	123	64
222	133	68
223	134	68
224	134	67
225	135	67
226	135	66
227	136	65
228	136	65
229	137	64
230	138	64
231	139	63
232	139	62
233	140	62
234	140	61
235	141	61
236	141	60
237	142	59
238	142	59
239	144	58
240	144	58
241	145	57
242	147	57
243	147	56
244	148	56
245	149	55
246	150	55
247	150	54
248	151	54
249	152	53
250	152	52
251	153	52
252	153	51
253	154	51
254	154	50
255	155	49
256	156	49
257	157	48
258	158	48
259	158	47
260	159	47
261	159	46
262	160	45
263	160	45
264	161	44
265	161	43
266	162	43
267	162	42
268	163	41
269	163	41
270	164	40
271	164	39
272	164	39
273	165	38
274	166	38
275	166	37
276	167	36
277	167	36
278	168	35
279	169	35
280	169	34
281	170	33
282	170	33
283	171	32
284	178	33
285	178	32
286	180	32
287	181	31
288	181	30
289	182	30
290	182	29
291	183	28
292	183	28
293	184	27
294	184	26
295	185	26
296	185	25
297	186	24
298	186	24
299	186	23
300	187	23
301	187	22
302	188	21
303	188	21
304	189	20
305	189	19
306	190	19
307	190	18
308	191	17
309	191	17
310	192	16
311	192	16
312	193	15
313	193	14
314	194	14
315	195	13
316	195	12
317	196	12
318	196	11
319	197	11
320	197	10
321	198	9
322	198	9
323	199	8
324	199	7
325	200	7
326	200	6
327	201	6
328	201	5
329	202	4
330	202	4
331	203	3
332	203	3
333	204	2
334	205	1
335	205	1
336	206	0
337	206	0
</file>

<file path="core.py">
from fastapi import FastAPI
from flask_bcrypt import Bcrypt

app = FastAPI(title="API do Projeto Daruma")

# Instancia o Bcrypt diretamente, sem associar a um app Flask.
bcrypt = Bcrypt()
</file>

<file path="crud.py">
# crud.py
from sqlalchemy.orm import Session
import models
import auth

def get_user_by_username(db: Session, username: str):
    return db.query(models.User).filter(models.User.username == username).first()

def create_user(db: Session, user_schema: models.User):
    hashed_password = auth.get_password_hash(user_schema.password)
    db_user = models.User(username=user_schema.username, password_hash=hashed_password)
    db.add(db_user)
    db.commit()
    db.refresh(db_user)
    return db_user
</file>

<file path="database.py">
# database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os

SQLALCHEMY_DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://user:password@db:5432/daruma_db')

engine = create_engine(SQLALCHEMY_DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# Dependency para obter a sessão do DB
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
</file>

<file path="Dockerfile">
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copia todos os arquivos da aplicação
COPY . .

EXPOSE 5000

# Comando para rodar a aplicação com Uvicorn (servidor ASGI para FastAPI)
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "5000"]
</file>

<file path="export_artifacts_target1.py">
# export_artifacts_target1.py

import pandas as pd
import numpy as np
import os
import pickle
import joblib
import optuna
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import r2_score
from catboost import CatBoostRegressor

print("="*80)
print("INICIANDO EXPORTAÇÃO DE ARTEFATOS PARA O TARGET 1")
print("="*80)

# --- 1. CONFIGURAÇÕES ---
ARTIFACTS_PATH = "ml_artifacts"
if not os.path.exists(ARTIFACTS_PATH):
    os.makedirs(ARTIFACTS_PATH)
TARGET = 'Target1'
RAW_DATA_FILE = 'JogadoresV1.xlsx'
RANDOM_STATE = 42

# --- 2. CARREGAMENTO E FEATURE ENGINEERING (Lógica do Notebook Fase 2) ---
print(f"\n[FASE 1] Carregando e processando dados de '{RAW_DATA_FILE}'...")
try:
    df = pd.read_excel(RAW_DATA_FILE)
except FileNotFoundError:
    print(f"❌ ERRO: Arquivo '{RAW_DATA_FILE}' não encontrado.")
    exit()

df.dropna(subset=[TARGET], inplace=True)

# Limpeza e conversão de tipos
if 'F0103' in df.columns:
    df['F0103'] = pd.to_numeric(df['F0103'].astype(str).str.replace(',', '.'), errors='coerce')

p_cols = [c for c in df.columns if c.startswith('P') and any(char.isdigit() for char in c)]
t_cols = [c for c in df.columns if c.startswith('T') and any(char.isdigit() for char in c)]
f_cols = [c for c in df.columns if c.startswith('F') and len(c) > 1 and any(char.isdigit() for char in c)]

for col in p_cols + t_cols + f_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')
    df[col].replace(-1, np.nan, inplace=True)

# Imputação com mediana
for col in p_cols + t_cols + f_cols:
    if df[col].isnull().sum() > 0:
        df[col].fillna(df[col].median(), inplace=True)

# Feature Engineering
if 'QtdHorasDormi' in df.columns and 'Acordar' in df.columns:
    df['sono_total'] = df['QtdHorasDormi']
    df['sono_x_acordar'] = df['QtdHorasDormi'] * df['Acordar']

if p_cols:
    df['P_mean'] = df[p_cols].mean(axis=1)
    df['P_std'] = df[p_cols].std(axis=1)

if t_cols:
    df['T_mean'] = df[t_cols].mean(axis=1)

# Agregações conceituais de F
f_sono = [c for c in f_cols if c.startswith('F07')]
if f_sono:
    df['F_sono_mean'] = df[f_sono].mean(axis=1)

print("✅ Dados processados e features criadas.")

# --- 3. SELEÇÃO DE FEATURES ---
print("\n[FASE 2] Selecionando as melhores features...")
numeric_features = df.select_dtypes(include=np.number).columns.tolist()
features_to_exclude = ['Target1', 'Target2', 'Target3']
feature_candidates = [f for f in numeric_features if f not in features_to_exclude]

X_temp = df[feature_candidates].fillna(0)
selector = VarianceThreshold(threshold=0.01)
selector.fit(X_temp)
feature_cols_var = X_temp.columns[selector.get_support()].tolist()

correlations = [(col, abs(df[col].corr(df[TARGET]))) for col in feature_cols_var]
correlations = [corr for corr in correlations if not np.isnan(corr[1])]
correlations.sort(key=lambda x: x[1], reverse=True)

TOP_K = 30
selected_features = [col for col, _ in correlations[:TOP_K]]

# Interações
top3_features = selected_features[:3]
for i, f1 in enumerate(top3_features):
    for f2 in top3_features[i+1:]:
        interaction_name = f'{f1}_X_{f2}'
        df[interaction_name] = df[f1] * df[f2]
        selected_features.append(interaction_name)

selected_features = list(dict.fromkeys(selected_features))
print(f"✅ {len(selected_features)} features finais selecionadas para {TARGET}.")

# --- 4. PREPARAÇÃO FINAL E SALVAMENTO DE ARTEFATOS ---
X = df[selected_features]
y = df[TARGET]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)

# Scaler
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
joblib.dump(scaler, f'{ARTIFACTS_PATH}/scaler_{TARGET.lower()}.pkl')
print(f"💾 Scaler para {TARGET} salvo em: {ARTIFACTS_PATH}/scaler_{TARGET.lower()}.pkl")

# Lista de Features
with open(f'{ARTIFACTS_PATH}/features_{TARGET.lower()}.pkl', 'wb') as f:
    pickle.dump(selected_features, f)
print(f"💾 Lista de features para {TARGET} salva em: {ARTIFACTS_PATH}/features_{TARGET.lower()}.pkl")

# --- 5. OTIMIZAÇÃO E TREINAMENTO DO MODELO (Lógica do Notebook Fase 3) ---
print(f"\n[FASE 3] Otimizando e treinando o modelo para {TARGET}...")

def objective_t1(trial):
    params = {
        'iterations': 500,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'depth': trial.suggest_int('depth', 3, 6),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 10, 100, log=True),
        'verbose': False,
        'random_seed': RANDOM_STATE
    }
    model = CatBoostRegressor(**params)
    model.fit(X_train_scaled, y_train, eval_set=[(scaler.transform(X_test), y_test)], early_stopping_rounds=50, verbose=False)
    preds = model.predict(scaler.transform(X_test))
    return r2_score(y_test, preds)

study = optuna.create_study(direction='maximize')
study.optimize(objective_t1, n_trials=50) # 50 trials for a good balance
best_params = study.best_params
best_params['iterations'] = 500 # Re-set iterations
best_params['verbose'] = False
best_params['random_seed'] = RANDOM_STATE

final_model = CatBoostRegressor(**best_params)
final_model.fit(X_train_scaled, y_train)

print(f"✅ Modelo {TARGET} treinado com R² de {study.best_value:.4f} na otimização.")

# Salvamento do Modelo
joblib.dump(final_model, f'{ARTIFACTS_PATH}/modelo_{TARGET.lower()}.pkl')
print(f"💾 Modelo para {TARGET} salvo em: {ARTIFACTS_PATH}/modelo_{TARGET.lower()}.pkl")
print("\n--- Concluído para Target 1 ---")
</file>

<file path="export_artifacts_target2.py">
# export_artifacts_target2.py

import pandas as pd
import numpy as np
import os
import pickle
import joblib
import optuna
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import RobustScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.linear_model import Ridge
from catboost import CatBoostRegressor
import lightgbm as lgb
import xgboost as xgb

print("="*80)
print("INICIANDO EXPORTAÇÃO DE ARTEFATOS PARA O TARGET 2")
print("="*80)

# --- 1. CONFIGURAÇÕES ---
ARTIFACTS_PATH = "ml_artifacts"
if not os.path.exists(ARTIFACTS_PATH):
    os.makedirs(ARTIFACTS_PATH)
TARGET = 'Target2'
RAW_DATA_FILE = 'JogadoresV1.xlsx'
RANDOM_STATE = 42

# --- 2. CARREGAMENTO E FEATURE ENGINEERING ---
print(f"\n[FASE 1] Carregando e processando dados de '{RAW_DATA_FILE}'...")
try:
    df = pd.read_excel(RAW_DATA_FILE)
except FileNotFoundError:
    print(f"❌ ERRO: Arquivo '{RAW_DATA_FILE}' não encontrado.")
    exit()

df.dropna(subset=[TARGET], inplace=True)

# Limpeza e FE simples
if 'F0103' in df.columns:
    df['F0103'] = pd.to_numeric(df['F0103'].astype(str).str.replace(',', '.'), errors='coerce')
if 'QtdHorasDormi' in df.columns and 'Acordar' in df.columns:
    df['sono_total'] = df['QtdHorasDormi']
    df['sono_x_acordar'] = df['QtdHorasDormi'] * df['Acordar']

p_cols = [c for c in df.columns if c.startswith('P') and any(char.isdigit() for char in c)]
t_cols = [c for c in df.columns if c.startswith('T') and any(char.isdigit() for char in c)]
f_cols = [c for c in df.columns if c.startswith('F') and len(c) > 1 and any(char.isdigit() for char in c)]

for col in p_cols + t_cols + f_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')

if p_cols:
    df['P_mean'] = df[p_cols].mean(axis=1)
if t_cols:
    df['T_mean'] = df[t_cols].mean(axis=1)
if f_cols:
    df['F_mean'] = df[f_cols].mean(axis=1)

# Imputação final antes de separar
numeric_cols = df.select_dtypes(include=np.number).columns
for col in numeric_cols:
    df[col].fillna(df[col].median(), inplace=True)

print("✅ Dados processados e features base criadas.")

# --- 3. PREPARAÇÃO E SEPARAÇÃO DE DADOS ---
features_to_exclude = ['Target1', 'Target2', 'Target3', 'Código de Acesso', 'Data/Hora Último']
initial_features = [col for col in df.columns if col not in features_to_exclude and pd.api.types.is_numeric_dtype(df[col])]

X = df[initial_features]
y = df[TARGET]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)

# --- 4. FEATURE ENGINEERING AVANÇADA E SELEÇÃO ---
print("\n[FASE 2] Criando features polinomiais e selecionando as melhores...")
# Features Polinomiais
rf_poly_selector = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)
rf_poly_selector.fit(X_train, y_train)
importances = pd.Series(rf_poly_selector.feature_importances_, index=X_train.columns).sort_values(ascending=False)
top_features_for_poly = importances.head(15).index.tolist()

poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)
X_train_poly = poly.fit_transform(X_train[top_features_for_poly])
poly_names = [f"poly_{name}" for name in poly.get_feature_names_out(top_features_for_poly)]
X_train_poly_df = pd.DataFrame(X_train_poly, columns=poly_names, index=X_train.index)
X_train_expanded = X_train.join(X_train_poly_df)

# Seleção Híbrida
correlations = X_train_expanded.corrwith(y_train).abs().sort_values(ascending=False)
top_corr_features = correlations.head(60).index.tolist()
rf_final_selector = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)
rf_final_selector.fit(X_train_expanded, y_train)
importances_final = pd.Series(rf_final_selector.feature_importances_, index=X_train_expanded.columns).sort_values(ascending=False)
top_rf_features = importances_final.head(60).index.tolist()

final_feature_list = list(set(top_corr_features + top_rf_features))
X_train_selected = X_train_expanded[final_feature_list]

print(f"✅ {len(final_feature_list)} features finais selecionadas para {TARGET}.")

# --- 5. SALVAMENTO DE ARTEFATOS DE PRÉ-PROCESSAMENTO ---
# Scaler
scaler = RobustScaler()
scaler.fit(X_train_selected)
joblib.dump(scaler, f'{ARTIFACTS_PATH}/scaler_{TARGET.lower()}.pkl')
print(f"💾 Scaler para {TARGET} salvo.")

# Transformador Polinomial e sua lista de features
joblib.dump(poly, f'{ARTIFACTS_PATH}/poly_transformer_{TARGET.lower()}.pkl')
with open(f'{ARTIFACTS_PATH}/poly_features_list_{TARGET.lower()}.pkl', 'wb') as f:
    pickle.dump(top_features_for_poly, f)
print(f"💾 Transformador polinomial e lista de features para {TARGET} salvos.")

# Lista final de Features
with open(f'{ARTIFACTS_PATH}/features_{TARGET.lower()}.pkl', 'wb') as f:
    pickle.dump(final_feature_list, f)
print(f"💾 Lista final de features para {TARGET} salva.")


# --- 6. OTIMIZAÇÃO E TREINAMENTO DO MODELO STACKING ---
print(f"\n[FASE 3] Otimizando modelos base e treinando o Stacking para {TARGET}...")

def tune_model(model_name, X, y):
    def objective(trial):
        if model_name == 'catboost':
            params = {'iterations': trial.suggest_int('iterations', 100, 500), 'depth': trial.suggest_int('depth', 3, 7), 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True), 'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2, 20, log=True), 'verbose': 0}
            model = CatBoostRegressor(**params, random_state=RANDOM_STATE)
        # Adicione lgb e xgb se necessário
        score = cross_val_score(model, X, y, cv=3, scoring='r2', n_jobs=-1).mean()
        return score
    
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=30)
    print(f"  - Melhor R² CV para {model_name}: {study.best_value:.4f}")
    return study.best_params

best_catboost_params = tune_model('catboost', scaler.transform(X_train_selected), y_train)

base_models = [
    ('catboost', CatBoostRegressor(**best_catboost_params, verbose=0, random_state=RANDOM_STATE)),
    ('random_forest', RandomForestRegressor(n_estimators=150, random_state=RANDOM_STATE))
]
meta_model = Ridge(random_state=RANDOM_STATE)
stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1)

stacking_model.fit(scaler.transform(X_train_selected), y_train)
print(f"✅ Modelo Stacking para {TARGET} treinado.")

# Salvamento do Modelo
joblib.dump(stacking_model, f'{ARTIFACTS_PATH}/modelo_{TARGET.lower()}.pkl')
print(f"💾 Modelo para {TARGET} salvo em: {ARTIFACTS_PATH}/modelo_{TARGET.lower()}.pkl")
print("\n--- Concluído para Target 2 ---")
</file>

<file path="export_artifacts_target3.py">
# export_artifacts_target3.py

import pandas as pd
import numpy as np
import os
import pickle
import joblib
import optuna
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import RobustScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.linear_model import Ridge
from catboost import CatBoostRegressor
import lightgbm as lgb
import xgboost as xgb

print("="*80)
print("INICIANDO EXPORTAÇÃO DE ARTEFATOS PARA O TARGET 3")
print("="*80)

# --- 1. CONFIGURAÇÕES ---
ARTIFACTS_PATH = "ml_artifacts"
if not os.path.exists(ARTIFACTS_PATH):
    os.makedirs(ARTIFACTS_PATH)
TARGET = 'Target3' # <- ALTERADO
RAW_DATA_FILE = 'JogadoresV1.xlsx'
RANDOM_STATE = 42

# --- 2. CARREGAMENTO E FEATURE ENGINEERING ---
print(f"\n[FASE 1] Carregando e processando dados de '{RAW_DATA_FILE}'...")
try:
    df = pd.read_excel(RAW_DATA_FILE)
except FileNotFoundError:
    print(f"❌ ERRO: Arquivo '{RAW_DATA_FILE}' não encontrado.")
    exit()

df.dropna(subset=[TARGET], inplace=True)

# Limpeza e FE simples
if 'F0103' in df.columns:
    df['F0103'] = pd.to_numeric(df['F0103'].astype(str).str.replace(',', '.'), errors='coerce')
if 'QtdHorasDormi' in df.columns and 'Acordar' in df.columns:
    df['sono_total'] = df['QtdHorasDormi']
    df['sono_x_acordar'] = df['QtdHorasDormi'] * df['Acordar']

p_cols = [c for c in df.columns if c.startswith('P') and any(char.isdigit() for char in c)]
t_cols = [c for c in df.columns if c.startswith('T') and any(char.isdigit() for char in c)]
f_cols = [c for c in df.columns if c.startswith('F') and len(c) > 1 and any(char.isdigit() for char in c)]

for col in p_cols + t_cols + f_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')

if p_cols:
    df['P_mean'] = df[p_cols].mean(axis=1)
if t_cols:
    df['T_mean'] = df[t_cols].mean(axis=1)
if f_cols:
    df['F_mean'] = df[f_cols].mean(axis=1)

# Imputação final antes de separar
numeric_cols = df.select_dtypes(include=np.number).columns
for col in numeric_cols:
    df[col].fillna(df[col].median(), inplace=True)

print("✅ Dados processados e features base criadas.")

# --- 3. PREPARAÇÃO E SEPARAÇÃO DE DADOS ---
features_to_exclude = ['Target1', 'Target2', 'Target3', 'Código de Acesso', 'Data/Hora Último']
initial_features = [col for col in df.columns if col not in features_to_exclude and pd.api.types.is_numeric_dtype(df[col])]

X = df[initial_features]
y = df[TARGET]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)

# --- 4. FEATURE ENGINEERING AVANÇADA E SELEÇÃO ---
print("\n[FASE 2] Criando features polinomiais e selecionando as melhores...")
# Features Polinomiais
rf_poly_selector = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)
rf_poly_selector.fit(X_train, y_train)
importances = pd.Series(rf_poly_selector.feature_importances_, index=X_train.columns).sort_values(ascending=False)
top_features_for_poly = importances.head(15).index.tolist()

poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)
X_train_poly = poly.fit_transform(X_train[top_features_for_poly])
poly_names = [f"poly_{name}" for name in poly.get_feature_names_out(top_features_for_poly)]
X_train_poly_df = pd.DataFrame(X_train_poly, columns=poly_names, index=X_train.index)
X_train_expanded = X_train.join(X_train_poly_df)

# Seleção Híbrida
correlations = X_train_expanded.corrwith(y_train).abs().sort_values(ascending=False)
top_corr_features = correlations.head(60).index.tolist()
rf_final_selector = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)
rf_final_selector.fit(X_train_expanded, y_train)
importances_final = pd.Series(rf_final_selector.feature_importances_, index=X_train_expanded.columns).sort_values(ascending=False)
top_rf_features = importances_final.head(60).index.tolist()

final_feature_list = list(set(top_corr_features + top_rf_features))
X_train_selected = X_train_expanded[final_feature_list]

print(f"✅ {len(final_feature_list)} features finais selecionadas para {TARGET}.")

# --- 5. SALVAMENTO DE ARTEFATOS DE PRÉ-PROCESSAMENTO ---
# Scaler
scaler = RobustScaler()
scaler.fit(X_train_selected)
joblib.dump(scaler, f'{ARTIFACTS_PATH}/scaler_{TARGET.lower()}.pkl') # <- NOME ALTERADO
print(f"💾 Scaler para {TARGET} salvo.")

# Transformador Polinomial e sua lista de features
joblib.dump(poly, f'{ARTIFACTS_PATH}/poly_transformer_{TARGET.lower()}.pkl') # <- NOME ALTERADO
with open(f'{ARTIFACTS_PATH}/poly_features_list_{TARGET.lower()}.pkl', 'wb') as f: # <- NOME ALTERADO
    pickle.dump(top_features_for_poly, f)
print(f"💾 Transformador polinomial e lista de features para {TARGET} salvos.")

# Lista final de Features
with open(f'{ARTIFACTS_PATH}/features_{TARGET.lower()}.pkl', 'wb') as f: # <- NOME ALTERADO
    pickle.dump(final_feature_list, f)
print(f"💾 Lista final de features para {TARGET} salva.")


# --- 6. OTIMIZAÇÃO E TREINAMENTO DO MODELO STACKING ---
print(f"\n[FASE 3] Otimizando modelos base e treinando o Stacking para {TARGET}...")

def tune_model(model_name, X, y):
    def objective(trial):
        if model_name == 'catboost':
            params = {'iterations': trial.suggest_int('iterations', 100, 500), 'depth': trial.suggest_int('depth', 3, 7), 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True), 'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2, 20, log=True), 'verbose': 0}
            model = CatBoostRegressor(**params, random_state=RANDOM_STATE)
        score = cross_val_score(model, X, y, cv=3, scoring='r2', n_jobs=-1).mean()
        return score
    
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=30)
    print(f"  - Melhor R² CV para {model_name}: {study.best_value:.4f}")
    return study.best_params

best_catboost_params = tune_model('catboost', scaler.transform(X_train_selected), y_train)

base_models = [
    ('catboost', CatBoostRegressor(**best_catboost_params, verbose=0, random_state=RANDOM_STATE)),
    ('random_forest', RandomForestRegressor(n_estimators=150, random_state=RANDOM_STATE))
]
meta_model = Ridge(random_state=RANDOM_STATE)
stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1)

stacking_model.fit(scaler.transform(X_train_selected), y_train)
print(f"✅ Modelo Stacking para {TARGET} treinado.")

# Salvamento do Modelo
joblib.dump(stacking_model, f'{ARTIFACTS_PATH}/modelo_{TARGET.lower()}.pkl') # <- NOME ALTERADO
print(f"💾 Modelo para {TARGET} salvo em: {ARTIFACTS_PATH}/modelo_{TARGET.lower()}.pkl")
print("\n--- Concluído para Target 3 ---")
</file>

<file path="export_clustering_artifacts.py">
# =============================================================================
# EXPORT DE ARTEFATOS DE CLUSTERING
# =============================================================================
# Treina os modelos de clustering UMA VEZ e salva para uso na API

import pandas as pd
import numpy as np
import warnings
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import joblib
import pickle
import os

warnings.filterwarnings('ignore')

ARTIFACTS_PATH = 'ml_artifacts'
os.makedirs(ARTIFACTS_PATH, exist_ok=True)

print("=" * 100)
print("🧬 TREINAMENTO DE CLUSTERING - ANÁLISE DE PERFIS".center(100))
print("=" * 100)

# =============================================================================
# CARREGAMENTO DOS DADOS
# =============================================================================

df_raw = pd.read_excel('JogadoresV3.xlsx')
df = df_raw.copy()

print(f"\n✅ Dados carregados: {len(df)} linhas")

# Converter F0103
if 'F0103' in df.columns:
    df['F0103'] = pd.to_numeric(df['F0103'].astype(str).str.replace(',', '.'), errors='coerce')

# Identificar colunas
p_cols = [c for c in df.columns if c.startswith('P') and any(char.isdigit() for char in c)]
t_cols = [c for c in df.columns if c.startswith('T') and any(char.isdigit() for char in c)]
f_cols = [c for c in df.columns if c.startswith('F') and len(c) > 1 and any(char.isdigit() for char in c)]

print(f"\n📊 Colunas identificadas:")
print(f"  • Performance (P): {len(p_cols)}")
print(f"  • Tempo (T): {len(t_cols)}")
print(f"  • Formulários (F): {len(f_cols)}")

# =============================================================================
# PRÉ-PROCESSAMENTO
# =============================================================================

print("\n🔧 Pré-processamento...")

# Tratar valores -1 e NaN
for col in p_cols + t_cols + f_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce').replace(-1, np.nan)
        if df[col].isnull().sum() > 0:
            df[col].fillna(df[col].median(), inplace=True)

# Criar features para clustering
df['P_mean'] = df[p_cols].mean(axis=1)
df['P_std'] = df[p_cols].std(axis=1)
df['P_max'] = df[p_cols].max(axis=1)
df['T_mean'] = df[t_cols].mean(axis=1)
df['T_total'] = df[t_cols].sum(axis=1)

f_sono = [c for c in f_cols if c.startswith('F07')]
if f_sono:
    df['F_sono_mean'] = df[f_sono].mean(axis=1)

f_final = [c for c in f_cols if c.startswith('F11')]
if f_final:
    df['F_final_mean'] = df[f_final].mean(axis=1)

# Features selecionadas
cluster_features = ['P_mean', 'P_std', 'T_mean', 'F_sono_mean', 'F_final_mean']
cluster_features = [f for f in cluster_features if f in df.columns]

print(f"  ✅ {len(cluster_features)} features criadas: {cluster_features}")

X_cluster = df[cluster_features].fillna(df[cluster_features].median())

# =============================================================================
# NORMALIZAÇÃO
# =============================================================================

print("\n📏 Normalizando dados...")

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_cluster)

print("  ✅ Scaler treinado")

# =============================================================================
# CLUSTERING (K-MEANS)
# =============================================================================

print("\n🎯 Treinando K-Means (2 clusters)...")

kmeans = KMeans(n_clusters=2, random_state=42, n_init=20, max_iter=300)
clusters = kmeans.fit_predict(X_scaled)

silhouette = silhouette_score(X_scaled, clusters)

print(f"  ✅ K-Means treinado")
print(f"  📊 Silhouette Score: {silhouette:.3f}")

# Nomear clusters baseado em performance média
cluster_0_p_mean = df[clusters == 0]['P_mean'].mean()
cluster_1_p_mean = df[clusters == 1]['P_mean'].mean()

if cluster_0_p_mean > cluster_1_p_mean:
    cluster_names = {0: "Alto Desempenho", 1: "Iniciante"}
else:
    cluster_names = {1: "Alto Desempenho", 0: "Iniciante"}

print(f"\n📋 Clusters identificados:")
for cluster_id in [0, 1]:
    mask = clusters == cluster_id
    count = np.sum(mask)
    percentage = count / len(clusters) * 100
    print(f"  • Cluster {cluster_id} ({cluster_names[cluster_id]}): {count} jogadores ({percentage:.1f}%)")

# =============================================================================
# PCA (VISUALIZAÇÃO)
# =============================================================================

print("\n🔬 Treinando PCA (2 componentes)...")

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

variance_explained = pca.explained_variance_ratio_

print(f"  ✅ PCA treinado")
print(f"  📊 Variância explicada:")
print(f"     - PC1: {variance_explained[0]*100:.1f}%")
print(f"     - PC2: {variance_explained[1]*100:.1f}%")
print(f"     - Total: {sum(variance_explained)*100:.1f}%")

# =============================================================================
# SALVAR ARTEFATOS
# =============================================================================

print("\n💾 Salvando artefatos...")

# 1. Modelo K-Means
joblib.dump(kmeans, f'{ARTIFACTS_PATH}/kmeans_model.pkl')
print(f"  ✅ K-Means salvo: {ARTIFACTS_PATH}/kmeans_model.pkl")

# 2. Modelo PCA
joblib.dump(pca, f'{ARTIFACTS_PATH}/pca_model.pkl')
print(f"  ✅ PCA salvo: {ARTIFACTS_PATH}/pca_model.pkl")

# 3. Scaler
joblib.dump(scaler, f'{ARTIFACTS_PATH}/scaler_cluster.pkl')
print(f"  ✅ Scaler salvo: {ARTIFACTS_PATH}/scaler_cluster.pkl")

# 4. Lista de features
with open(f'{ARTIFACTS_PATH}/cluster_features.pkl', 'wb') as f:
    pickle.dump(cluster_features, f)
print(f"  ✅ Features salvas: {ARTIFACTS_PATH}/cluster_features.pkl")

# 5. Nomes dos clusters
with open(f'{ARTIFACTS_PATH}/cluster_names.pkl', 'wb') as f:
    pickle.dump(cluster_names, f)
print(f"  ✅ Nomes dos clusters salvos: {ARTIFACTS_PATH}/cluster_names.pkl")

print("\n" + "=" * 100)
print("✅ CLUSTERING - ARTEFATOS SALVOS COM SUCESSO!".center(100))
print("=" * 100)

print(f"\n📦 Arquivos gerados em: {ARTIFACTS_PATH}/")
print(f"  • kmeans_model.pkl")
print(f"  • pca_model.pkl")
print(f"  • scaler_cluster.pkl")
print(f"  • cluster_features.pkl")
print(f"  • cluster_names.pkl")

print("\n💡 Próximo passo: Adicionar o carregamento desses artefatos no main.py")
</file>

<file path="export_hibrido_target1.py">
# =============================================================================
# EXPORT HÍBRIDO - TARGET 1 (R1)
# =============================================================================
# Este script replica a SEÇÃO 2 do notebook híbrido definitivo
# Treina o modelo R1 e salva os artefatos necessários para a API

import pandas as pd
import numpy as np
import warnings
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import VarianceThreshold
from catboost import CatBoostRegressor
import optuna
import joblib
import pickle
import os

warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)

# Configuração de caminhos
ARTIFACTS_PATH = 'ml_artifacts'
os.makedirs(ARTIFACTS_PATH, exist_ok=True)

print("=" * 100)
print("🎯 TREINAMENTO TARGET 1 (R1) - MODELO HÍBRIDO".center(100))
print("=" * 100)

# =============================================================================
# CARREGAMENTO E PRÉ-PROCESSAMENTO
# =============================================================================

df_raw = pd.read_excel('JogadoresV3.xlsx')
df = df_raw.copy()

print(f"\n✅ Dados carregados: {len(df)} linhas")

# Converter F0103
if 'F0103' in df.columns:
    df['F0103'] = pd.to_numeric(df['F0103'].astype(str).str.replace(',', '.'), errors='coerce')

# Identificar colunas P, T, F
p_cols = [col for col in df.columns if col.startswith('P') and any(c.isdigit() for c in col)]
t_cols = [col for col in df.columns if col.startswith('T') and any(c.isdigit() for c in col)]
f_cols = [col for col in df.columns if col.startswith('F') and len(col) > 1 and any(c.isdigit() for c in col)]

# Tratar colunas duplicadas
print("\n[1/6] Tratando colunas duplicadas...")
cols = pd.Series(df.columns)
duplicated_cols = cols[cols.duplicated()].unique()

if len(duplicated_cols) > 0:
    for dup in duplicated_cols:
        indices = cols[cols == dup].index.tolist()
        for i, idx in enumerate(indices):
            cols.iloc[idx] = f'{dup}_{i}'
    df.columns = cols
    # Atualizar listas de colunas
    p_cols = [col for col in df.columns if col.startswith('P') and any(c.isdigit() for c in col)]
    t_cols = [col for col in df.columns if col.startswith('T') and any(c.isdigit() for c in col)]
    f_cols = [col for col in df.columns if col.startswith('F') and len(col) > 1 and any(c.isdigit() for c in col)]
    print(f"  ✅ Colunas duplicadas renomeadas")
else:
    print("  ✅ Sem duplicatas")

# Converter para numérico
for col in p_cols + t_cols + f_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

# Remover outliers extremos
numeric_cols = [col for col in df.select_dtypes(include=[np.number]).columns if 'Target' not in col]
for col in numeric_cols:
    if col in df.columns:
        df.loc[df[col] < -100, col] = np.nan
        if df[col].max() > 10000:
            df.loc[df[col] > 10000, col] = np.nan

# =============================================================================
# FEATURE ENGINEERING
# =============================================================================

print("\n[2/6] Criando features comportamentais...")

# Taxa de pulos (-1)
p_minus_ones = sum((df[col] == -1).sum() for col in p_cols if col in df.columns)
t_minus_ones = sum((df[col] == -1).sum() for col in t_cols if col in df.columns)

df['taxa_pulos_P'] = p_minus_ones / len(p_cols) if len(p_cols) > 0 else 0
df['taxa_pulos_T'] = t_minus_ones / len(t_cols) if len(t_cols) > 0 else 0
df['taxa_pulos_geral'] = (p_minus_ones + t_minus_ones) / (len(p_cols) + len(t_cols))

# Substituir -1 por NaN e preencher com mediana
for col in p_cols + t_cols + f_cols:
    if col in df.columns:
        df[col] = df[col].replace(-1, np.nan)
        df[col] = df[col].replace(-1.0, np.nan)
        if df[col].isnull().sum() > 0:
            df[col].fillna(df[col].median(), inplace=True)

print("\n[3/6] Feature engineering avançado...")

# Features de SONO
if 'QtdHorasDormi' in df.columns and 'Acordar' in df.columns:
    df['sono_total'] = df['QtdHorasDormi']
    df['sono_x_acordar'] = df['QtdHorasDormi'] * df['Acordar']
    df['sono_squared'] = df['QtdHorasDormi'] ** 2
    df['sono_irregular'] = np.abs(df['QtdHorasDormi'] - df['QtdHorasDormi'].median())
    print(f"  ✅ Sono: 4 features")

# Features de PERFORMANCE
if len(p_cols) > 0:
    df['P_mean'] = df[p_cols].mean(axis=1)
    df['P_std'] = df[p_cols].std(axis=1)
    df['P_min'] = df[p_cols].min(axis=1)
    df['P_max'] = df[p_cols].max(axis=1)
    df['P_range'] = df['P_max'] - df['P_min']
    df['P_late'] = df[['P09', 'P12', 'P13', 'P15']].mean(axis=1) if all(c in df.columns for c in ['P09', 'P12', 'P13', 'P15']) else 0
    df['P_early'] = df[['P01', 'P02', 'P03', 'P04']].mean(axis=1) if all(c in df.columns for c in ['P01', 'P02', 'P03', 'P04']) else 0
    print(f"  ✅ Performance: 7 features")

# Features de TEMPO
if len(t_cols) > 0:
    df['T_mean'] = df[t_cols].mean(axis=1)
    df['T_std'] = df[t_cols].std(axis=1)
    df['T_min'] = df[t_cols].min(axis=1)
    df['T_max'] = df[t_cols].max(axis=1)
    print(f"  ✅ Tempo: 4 features")

# Features de FORMULÁRIOS
f_perfil = [c for c in f_cols if c.startswith('F01') or c.startswith('F02')]
if len(f_perfil) > 0:
    df['F_perfil_mean'] = df[f_perfil].mean(axis=1)
    df['F_perfil_std'] = df[f_perfil].std(axis=1)

f_sono = [c for c in f_cols if c.startswith('F07')]
if len(f_sono) > 0:
    df['F_sono_mean'] = df[f_sono].mean(axis=1)
    df['F_sono_std'] = df[f_sono].std(axis=1)

f_final = [c for c in f_cols if c.startswith('F11')]
if len(f_final) > 0:
    df['F_final_mean'] = df[f_final].mean(axis=1)
    df['F_final_std'] = df[f_final].std(axis=1)

df['F_mean_geral'] = df[f_cols].mean(axis=1)

# =============================================================================
# SELEÇÃO DE FEATURES
# =============================================================================

print("\n[4/6] Selecionando TOP features...")

TARGET = 'Target1'
feature_cols = [col for col in df.columns if col not in [TARGET, 'Código de Acesso', 'Data/Hora Último', 'Target2', 'Target3']
                and pd.api.types.is_numeric_dtype(df[col])]

X = df[feature_cols].fillna(0)
y = df[TARGET]

# Variance Threshold
selector = VarianceThreshold(threshold=0.01)
selector.fit(X)
feature_cols = X.columns[selector.get_support()].tolist()
X = df[feature_cols]

# Correlação com target
correlations = []
for col in feature_cols:
    corr = abs(df[col].corr(df[TARGET]))
    if not np.isnan(corr):
        correlations.append((col, corr))

correlations.sort(key=lambda x: x[1], reverse=True)

# TOP 30 features
TOP_K = min(30, len(correlations))
selected_features_r1 = [col for col, _ in correlations[:TOP_K]]

print(f"  ✅ {TOP_K} features selecionadas")

# Criar interações entre TOP 3
top3_features = [col for col, _ in correlations[:3]]
interaction_features = []

for i, f1 in enumerate(top3_features):
    for f2 in top3_features[i+1:]:
        interaction_name = f'{f1}_X_{f2}'
        df[interaction_name] = df[f1] * df[f2]
        interaction_features.append(interaction_name)

selected_features_r1.extend(interaction_features)
selected_features_r1 = list(dict.fromkeys(selected_features_r1))

print(f"  Total com interações: {len(selected_features_r1)}")

# =============================================================================
# PREPARAÇÃO DOS DADOS
# =============================================================================

print("\n[5/6] Preparando dados para treinamento...")

# Scaler
scaler_r1 = RobustScaler()
X_final = df[selected_features_r1].copy().fillna(0)
X_scaled = scaler_r1.fit_transform(X_final)

X_r1 = X_scaled
y_r1 = df[TARGET].values

# Remover NaNs
valid_idx = ~np.isnan(y_r1)
X_r1 = X_r1[valid_idx]
y_r1 = y_r1[valid_idx]

# Split
X_train_r1, X_test_r1, y_train_r1, y_test_r1 = train_test_split(X_r1, y_r1, test_size=0.25, random_state=42)

print(f"  Treino: {len(X_train_r1)} | Teste: {len(X_test_r1)}")

# =============================================================================
# OTIMIZAÇÃO E TREINAMENTO
# =============================================================================

print("\n[6/6] Otimizando hiperparâmetros (100 trials)...")

def objective_r1(trial):
    params = {
        'iterations': 500,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'depth': trial.suggest_int('depth', 3, 6),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 10, 100, log=True),
        'border_count': trial.suggest_int('border_count', 32, 128),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.5, 1.0),
        'random_strength': trial.suggest_float('random_strength', 0.5, 2.0),
        'verbose': False,
        'random_seed': 42
    }
    model = CatBoostRegressor(**params)
    scores = cross_val_score(model, X_train_r1, y_train_r1, cv=5, scoring='r2')
    return scores.mean()

study_r1 = optuna.create_study(direction='maximize')
study_r1.optimize(objective_r1, n_trials=100, show_progress_bar=True)

best_params_r1 = study_r1.best_params
best_params_r1['iterations'] = 500
best_params_r1['verbose'] = False
best_params_r1['random_seed'] = 42

print(f"\n✅ Melhor R² CV: {study_r1.best_value:.4f}")

# Treinar modelo final
print("\n🔧 Treinando modelo final...")
model_r1 = CatBoostRegressor(**best_params_r1)
model_r1.fit(X_train_r1, y_train_r1, verbose=False)

# =============================================================================
# SALVAR ARTEFATOS
# =============================================================================

print("\n💾 Salvando artefatos...")

# Salvar modelo
joblib.dump(model_r1, f'{ARTIFACTS_PATH}/modelo_target1.pkl')
print(f"  ✅ Modelo salvo: {ARTIFACTS_PATH}/modelo_target1.pkl")

# Salvar scaler
joblib.dump(scaler_r1, f'{ARTIFACTS_PATH}/scaler_target1.pkl')
print(f"  ✅ Scaler salvo: {ARTIFACTS_PATH}/scaler_target1.pkl")

# Salvar lista de features
with open(f'{ARTIFACTS_PATH}/features_target1.pkl', 'wb') as f:
    pickle.dump(selected_features_r1, f)
print(f"  ✅ Features salvas: {ARTIFACTS_PATH}/features_target1.pkl")

print("\n" + "=" * 100)
print("✅ TARGET 1 (R1) - TREINAMENTO COMPLETO!".center(100))
print("=" * 100)
print(f"\n📦 Artefatos salvos em: {ARTIFACTS_PATH}/")
print(f"  • modelo_target1.pkl")
print(f"  • scaler_target1.pkl")
print(f"  • features_target1.pkl")
</file>

<file path="export_hibrido_target2.py">
# =============================================================================
# EXPORT HÍBRIDO - TARGET 2 (R2)
# =============================================================================
# Este script replica a SEÇÃO 3 do notebook híbrido definitivo (CORRIGIDO!)
# Treina o ENSEMBLE de 3 modelos R2 e salva os artefatos necessários para a API

import pandas as pd
import numpy as np
import warnings
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import QuantileTransformer
from sklearn.feature_selection import VarianceThreshold
from catboost import CatBoostRegressor
import optuna
import joblib
import pickle
import os

warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)

# Configuração de caminhos
ARTIFACTS_PATH = 'ml_artifacts'
os.makedirs(ARTIFACTS_PATH, exist_ok=True)

print("=" * 100)
print("🎯 TREINAMENTO TARGET 2 (R2) - ENSEMBLE HÍBRIDO CORRIGIDO".center(100))
print("=" * 100)

# =============================================================================
# CARREGAMENTO E PRÉ-PROCESSAMENTO
# =============================================================================

df_raw = pd.read_excel('JogadoresV3.xlsx')
df = df_raw.copy()

TARGET = 'Target2'

print(f"\n✅ Dados carregados: {len(df)} linhas")

# Converter F0103
if 'F0103' in df.columns:
    df['F0103'] = pd.to_numeric(df['F0103'].astype(str).str.replace(',', '.'), errors='coerce')

# Identificar colunas P, T, F
p_cols = [col for col in df.columns if col.startswith('P') and any(c.isdigit() for c in col)]
t_cols = [col for col in df.columns if col.startswith('T') and any(c.isdigit() for c in col)]
f_cols = [col for col in df.columns if col.startswith('F') and len(col) > 1 and any(c.isdigit() for c in col)]

print("\n[1/5] Tratando valores -1 e NaN...")

# Converter para numérico e tratar -1
for col in p_cols + t_cols + f_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
        df[col] = df[col].replace(-1, np.nan)
        if df[col].isnull().sum() > 0:
            df[col].fillna(df[col].median(), inplace=True)

# =============================================================================
# FEATURE ENGINEERING MINIMALISTA
# =============================================================================

print("\n[2/5] Criando features minimalistas para R2...")

# Features de sono
if 'QtdHorasDormi' in df.columns and 'Acordar' in df.columns:
    df['sono_total'] = df['QtdHorasDormi']
    df['acordar'] = df['Acordar']

# Features de formulário sono
f_sono = [c for c in f_cols if c.startswith('F07')]
if len(f_sono) > 0:
    df['F_sono_mean'] = df[f_sono].mean(axis=1)

# Features de formulário final
f_final = [c for c in f_cols if c.startswith('F11')]
if len(f_final) > 0:
    df['F_final_mean'] = df[f_final].mean(axis=1)

# Features de performance
p_cols_exist = [c for c in p_cols if c in df.columns]
if len(p_cols_exist) > 0:
    df['P_mean'] = df[p_cols_exist].mean(axis=1)

# Preencher NaN remanescentes
numeric_cols = df.select_dtypes(include=np.number).columns
for col in numeric_cols:
    if df[col].isnull().sum() > 0:
        df[col].fillna(df[col].median(), inplace=True)

# =============================================================================
# SELEÇÃO DE FEATURES CONSERVADORA
# =============================================================================

print("\n[3/5] Seleção conservadora (≤12 features)...")

feature_cols = [col for col in df.columns if col not in [TARGET, 'Código de Acesso', 'Data/Hora Último', 'Target1', 'Target3']
                and pd.api.types.is_numeric_dtype(df[col])]

X_pre = df[feature_cols].fillna(0)
y = df[TARGET]

# Variance Threshold
selector = VarianceThreshold(threshold=0.01)
selector.fit(X_pre)
feature_cols_filtered = X_pre.columns[selector.get_support()].tolist()

# Correlação com target
correlations = []
for col in feature_cols_filtered:
    corr = df[col].corr(df[TARGET])
    if not np.isnan(corr):
        correlations.append((col, abs(corr)))

correlations.sort(key=lambda x: x[1], reverse=True)

# TOP 12 features
MAX_FEATURES = 12
selected_features_r2 = [col for col, _ in correlations[:MAX_FEATURES]]

print(f"  ✅ {len(selected_features_r2)} features selecionadas")

# Criar uma interação entre TOP 2
if len(selected_features_r2) >= 2:
    f1, f2 = selected_features_r2[0], selected_features_r2[1]
    df[f'{f1}_X_{f2}'] = df[f1] * df[f2]
    selected_features_r2.append(f'{f1}_X_{f2}')

# =============================================================================
# PREPARAÇÃO DOS DADOS
# =============================================================================

print("\n[4/5] Preparando dados...")

X_r2 = df[selected_features_r2].copy()
y_r2 = df[TARGET].values

# Remover NaNs
valid_idx = ~np.isnan(y_r2)
X_r2 = X_r2[valid_idx]
y_r2 = y_r2[valid_idx]

print(f"  Dados: {len(X_r2)} amostras × {len(selected_features_r2)} features")

# =============================================================================
# OTIMIZAÇÃO
# =============================================================================

print("\n[5/5] Otimização brutal (150 trials)...")

def objective_r2(trial):
    params = {
        'iterations': trial.suggest_int('iterations', 300, 700),
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.03, log=True),
        'depth': trial.suggest_int('depth', 2, 3),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 50, 300, log=True),
        'border_count': trial.suggest_int('border_count', 16, 48),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 0.5),
        'random_strength': trial.suggest_float('random_strength', 2.0, 5.0),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 15),
        'verbose': False,
        'random_seed': 42
    }

    X_tr_opt, X_te_opt, y_tr_opt, y_te_opt = train_test_split(X_r2, y_r2, test_size=0.25, random_state=42)
    scaler_temp = QuantileTransformer(output_distribution='normal', random_state=42)
    X_tr_scaled = scaler_temp.fit_transform(X_tr_opt)

    model = CatBoostRegressor(**params)
    scores = cross_val_score(model, X_tr_scaled, y_tr_opt, cv=5, scoring='r2')
    return scores.mean()

study_r2 = optuna.create_study(direction='maximize')
study_r2.optimize(objective_r2, n_trials=150, show_progress_bar=True)

best_params_r2 = study_r2.best_params
best_params_r2['verbose'] = False
best_params_r2['random_seed'] = 42

print(f"\n✅ Melhor R² CV: {study_r2.best_value:.4f}")

# =============================================================================
# TREINAMENTO DO ENSEMBLE (3 MODELOS) - VERSÃO CORRIGIDA
# =============================================================================

print("\n🚀 Treinando ensemble (3 modelos)...")
print("  ✅ Cada modelo treina com seed diferente (diversidade)")

# SPLIT BASE COMUM (para consistência do scaler)
X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(
    X_r2, y_r2, test_size=0.25, random_state=42
)

# SCALER BASE COMUM
scaler_base = QuantileTransformer(output_distribution='normal', random_state=42)
X_train_base_scaled = scaler_base.fit_transform(X_train_base)
X_test_base_scaled = scaler_base.transform(X_test_base)

models_r2 = []

for i, seed in enumerate([42, 123, 456], 1):
    print(f"\n  Treinando Modelo {i} (seed={seed})...")
    
    # Cada modelo treina com seed diferente para diversidade
    X_tr_div, X_te_div, y_tr_div, y_te_div = train_test_split(X_r2, y_r2, test_size=0.25, random_state=seed)
    scaler_div = QuantileTransformer(output_distribution='normal', random_state=42)
    X_tr_div_scaled = scaler_div.fit_transform(X_tr_div)
    
    # Treinar modelo
    params_i = best_params_r2.copy()
    params_i['random_seed'] = seed
    model_i = CatBoostRegressor(**params_i)
    model_i.fit(X_tr_div_scaled, y_tr_div, verbose=False)
    
    models_r2.append(model_i)
    print(f"  ✅ Modelo {i} treinado!")

# =============================================================================
# SALVAR ARTEFATOS
# =============================================================================

print("\n💾 Salvando artefatos...")

# Salvar os 3 modelos do ensemble
for i, model in enumerate(models_r2):
    joblib.dump(model, f'{ARTIFACTS_PATH}/modelo_target2_ensemble_{i}.pkl')
    print(f"  ✅ Modelo {i+1} salvo: {ARTIFACTS_PATH}/modelo_target2_ensemble_{i}.pkl")

# Salvar o scaler (QuantileTransformer)
joblib.dump(scaler_base, f'{ARTIFACTS_PATH}/scaler_target2.pkl')
print(f"  ✅ Scaler salvo: {ARTIFACTS_PATH}/scaler_target2.pkl")

# Salvar a lista de features
with open(f'{ARTIFACTS_PATH}/features_target2.pkl', 'wb') as f:
    pickle.dump(selected_features_r2, f)
print(f"  ✅ Features salvas: {ARTIFACTS_PATH}/features_target2.pkl")

print("\n" + "=" * 100)
print("✅ TARGET 2 (R2) - ENSEMBLE COMPLETO!".center(100))
print("=" * 100)
print(f"\n📦 Artefatos salvos em: {ARTIFACTS_PATH}/")
print(f"  • modelo_target2_ensemble_0.pkl")
print(f"  • modelo_target2_ensemble_1.pkl")
print(f"  • modelo_target2_ensemble_2.pkl")
print(f"  • scaler_target2.pkl (QuantileTransformer)")
print(f"  • features_target2.pkl")
print(f"\n💡 NOTA: A API fará a média das predições dos 3 modelos")
</file>

<file path="export_hibrido_target3.py">
# =============================================================================
# EXPORT HÍBRIDO - TARGET 3 (R3)
# =============================================================================
# Este script replica a SEÇÃO 4 do notebook híbrido definitivo (CORRIGIDO!)
# Treina o ENSEMBLE de 3 modelos R3 e salva os artefatos necessários para a API

import pandas as pd
import numpy as np
import warnings
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import RobustScaler
from catboost import CatBoostRegressor
import optuna
import joblib
import pickle
import os

warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)

# Configuração de caminhos
ARTIFACTS_PATH = 'ml_artifacts'
os.makedirs(ARTIFACTS_PATH, exist_ok=True)

print("=" * 100)
print("🎯 TREINAMENTO TARGET 3 (R3) - ENSEMBLE HÍBRIDO CORRIGIDO".center(100))
print("=" * 100)

# =============================================================================
# CARREGAMENTO E PRÉ-PROCESSAMENTO
# =============================================================================

df_raw = pd.read_excel('JogadoresV3.xlsx')
df = df_raw.copy()

TARGET = 'Target3'

print(f"\n✅ Dados carregados: {len(df)} linhas")

# Converter F0103
if 'F0103' in df.columns:
    df['F0103'] = pd.to_numeric(df['F0103'].astype(str).str.replace(',', '.'), errors='coerce')

# Identificar colunas P, T, F
p_cols = [col for col in df.columns if col.startswith('P') and any(c.isdigit() for c in col)]
t_cols = [col for col in df.columns if col.startswith('T') and any(c.isdigit() for c in col)]
f_cols = [col for col in df.columns if col.startswith('F') and len(col) > 1 and any(c.isdigit() for c in col)]

print("\n[1/5] Tratando valores -1 e NaN...")

# Converter para numérico e tratar -1
for col in p_cols + t_cols + f_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
        df[col] = df[col].replace(-1, np.nan)
        if df[col].isnull().sum() > 0:
            df[col].fillna(df[col].median(), inplace=True)

# =============================================================================
# FEATURE ENGINEERING
# =============================================================================

print("\n[2/5] Feature engineering para R3...")

# Features de Performance
df['P_mean'] = df[p_cols].mean(axis=1)
df['P_std'] = df[p_cols].std(axis=1)
df['P_late'] = df[['P09', 'P12', 'P13', 'P15']].mean(axis=1) if all(c in df.columns for c in ['P09', 'P12', 'P13', 'P15']) else 0
df['P_early'] = df[['P01', 'P02', 'P03', 'P04']].mean(axis=1) if all(c in df.columns for c in ['P01', 'P02', 'P03', 'P04']) else 0

# Features de Tempo
df['T_mean'] = df[t_cols].mean(axis=1)
df['T_std'] = df[t_cols].std(axis=1)

# Features de Sono
if 'QtdHorasSono' in df.columns:
    f_sono = [c for c in f_cols if '07' in c]
    df['F_sono_mean'] = df[f_sono].mean(axis=1)
    df['F_sono_std'] = df[f_sono].std(axis=1)
    df['F_sono_max'] = df[f_sono].max(axis=1)
    if 'Acordar' in df.columns:
        df['sono_x_acordar'] = df['QtdHorasSono'] * df['Acordar']
        df['acordar_squared'] = df['Acordar'] ** 2

# Features de Formulário Final
f_final = [c for c in f_cols if '11' in c]
df['F_final_mean'] = df[f_final].mean(axis=1)

print("  ✅ Features criadas!")

# =============================================================================
# SELEÇÃO DE FEATURES
# =============================================================================

print("\n[3/5] Seleção TOP 15 features...")

# Pool de features com correlação > 0.35
feature_pool = []
for col in df.columns:
    if col not in [TARGET, 'Código de Acesso', 'Target1', 'Target2'] and df[col].dtype in ['float64', 'int64']:
        corr = abs(df[col].corr(df[TARGET]))
        if not np.isnan(corr) and corr > 0.35:
            feature_pool.append((col, corr))

feature_pool.sort(key=lambda x: x[1], reverse=True)
selected_features_r3 = [f[0] for f in feature_pool[:15]]

print(f"  ✅ {len(selected_features_r3)} features selecionadas")

# Criar interação
if 'F1103' in selected_features_r3 and 'P_mean' in selected_features_r3:
    df['F1103_X_P_mean'] = df['F1103'] * df['P_mean']
    selected_features_r3.append('F1103_X_P_mean')

# =============================================================================
# PREPARAÇÃO DOS DADOS
# =============================================================================

print("\n[4/5] Preparando dados...")

X_r3 = df[selected_features_r3].fillna(df[selected_features_r3].median())
y_r3 = df[TARGET].values

print(f"  Dados: {len(X_r3)} amostras × {len(selected_features_r3)} features")

# =============================================================================
# OTIMIZAÇÃO
# =============================================================================

print("\n[5/5] Otimização (100 trials)...")

def objective_r3(trial):
    X_tr_opt, X_te_opt, y_tr_opt, y_te_opt = train_test_split(X_r3, y_r3, test_size=0.25, random_state=42)
    scaler_temp = RobustScaler()
    X_tr_scaled = scaler_temp.fit_transform(X_tr_opt)

    params = {
        'iterations': trial.suggest_int('iterations', 200, 800),
        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),
        'depth': 2,
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 120, 180),
        'border_count': trial.suggest_int('border_count', 16, 128),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),
        'random_strength': trial.suggest_float('random_strength', 0.5, 5),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 8, 15),
        'random_seed': 42,
        'verbose': False
    }
    model = CatBoostRegressor(**params)
    scores = cross_val_score(model, X_tr_scaled, y_tr_opt, cv=3, scoring='r2')
    return scores.mean()

study_r3 = optuna.create_study(direction='maximize')
study_r3.optimize(objective_r3, n_trials=100, show_progress_bar=True)

best_params_r3 = study_r3.best_params
best_params_r3['depth'] = 2
best_params_r3['verbose'] = False
best_params_r3['random_seed'] = 42

print(f"\n✅ Melhor R² CV: {study_r3.best_value:.4f}")

# =============================================================================
# TREINAMENTO DO ENSEMBLE (3 MODELOS) - VERSÃO CORRIGIDA
# =============================================================================

print("\n🚀 Treinando ensemble (3 modelos)...")
print("  ✅ Cada modelo treina com seed diferente (diversidade)")

# SPLIT BASE COMUM (para consistência do scaler)
X_train_base_r3, X_test_base_r3, y_train_base_r3, y_test_base_r3 = train_test_split(
    X_r3, y_r3, test_size=0.25, random_state=42
)

# SCALER BASE COMUM
scaler_base_r3 = RobustScaler()
X_train_base_r3_scaled = scaler_base_r3.fit_transform(X_train_base_r3)
X_test_base_r3_scaled = scaler_base_r3.transform(X_test_base_r3)

models_r3 = []

for i, seed in enumerate([42, 123, 456], 1):
    print(f"\n  Treinando Modelo {i} (seed={seed})...")
    
    # Cada modelo treina com seed diferente para diversidade
    X_tr_div, X_te_div, y_tr_div, y_te_div = train_test_split(X_r3, y_r3, test_size=0.25, random_state=seed)
    scaler_div = RobustScaler()
    X_tr_div_scaled = scaler_div.fit_transform(X_tr_div)
    
    # Treinar modelo
    params_i = best_params_r3.copy()
    params_i['random_seed'] = seed
    model_i = CatBoostRegressor(**params_i)
    model_i.fit(X_tr_div_scaled, y_tr_div, verbose=False)
    
    models_r3.append(model_i)
    print(f"  ✅ Modelo {i} treinado!")

# =============================================================================
# SALVAR ARTEFATOS
# =============================================================================

print("\n💾 Salvando artefatos...")

# Salvar os 3 modelos do ensemble
for i, model in enumerate(models_r3):
    joblib.dump(model, f'{ARTIFACTS_PATH}/modelo_target3_ensemble_{i}.pkl')
    print(f"  ✅ Modelo {i+1} salvo: {ARTIFACTS_PATH}/modelo_target3_ensemble_{i}.pkl")

# Salvar o scaler (RobustScaler)
joblib.dump(scaler_base_r3, f'{ARTIFACTS_PATH}/scaler_target3.pkl')
print(f"  ✅ Scaler salvo: {ARTIFACTS_PATH}/scaler_target3.pkl")

# Salvar a lista de features
with open(f'{ARTIFACTS_PATH}/features_target3.pkl', 'wb') as f:
    pickle.dump(selected_features_r3, f)
print(f"  ✅ Features salvas: {ARTIFACTS_PATH}/features_target3.pkl")

print("\n" + "=" * 100)
print("✅ TARGET 3 (R3) - ENSEMBLE COMPLETO!".center(100))
print("=" * 100)
print(f"\n📦 Artefatos salvos em: {ARTIFACTS_PATH}/")
print(f"  • modelo_target3_ensemble_0.pkl")
print(f"  • modelo_target3_ensemble_1.pkl")
print(f"  • modelo_target3_ensemble_2.pkl")
print(f"  • scaler_target3.pkl (RobustScaler)")
print(f"  • features_target3.pkl")
print(f"\n💡 NOTA: A API fará a média das predições dos 3 modelos")
</file>

<file path="main.py">
# main.py (COM MODELO HÍBRIDO - VERSÃO FINAL)

import os
import pickle
import joblib
import pandas as pd
import numpy as np
import shap
from fastapi import FastAPI, Depends, HTTPException, UploadFile, File, status
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError
from sqlalchemy import func
import crud
import models
import schemas
import auth
import database
from sklearn.ensemble import StackingRegressor
from flask_bcrypt import Bcrypt
from core import app
from io import BytesIO
import warnings
from pandas.errors import SettingWithCopyWarning
warnings.simplefilter(action="ignore", category=FutureWarning)

# Cria tabelas no DB (se não existirem) ao iniciar
try:
    models.Base.metadata.create_all(bind=database.engine)
except Exception as e:
    print(f"Aviso: Não foi possível criar tabelas do DB na inicialização (pode ser normal se já existirem): {e}")

# --- Carregamento de Artefatos de ML (ATUALIZADO PARA MODELO HÍBRIDO) ---
ARTIFACTS_PATH = os.getenv('ARTIFACTS_PATH', 'ml_artifacts')
MODELS, SCALERS, FEATURES, EXPLAINERS = {}, {}, {}, {}

try:
    # Target 1 (modelo único - mantém compatibilidade)
    MODELS['target1'] = joblib.load(f"{ARTIFACTS_PATH}/modelo_target1.pkl")
    SCALERS['target1'] = joblib.load(f"{ARTIFACTS_PATH}/scaler_target1.pkl")
    with open(f"{ARTIFACTS_PATH}/features_target1.pkl", "rb") as f:
        FEATURES['target1'] = pickle.load(f)
    EXPLAINERS['target1'] = shap.TreeExplainer(MODELS['target1'])

    # Targets 2 e 3 (ensemble de 3 modelos cada - NOVA ESTRUTURA)
    for target in ['target2', 'target3']:
        MODELS[target] = []
        for i in range(3): # Carrega os 3 modelos do ensemble
            model = joblib.load(f"{ARTIFACTS_PATH}/modelo_{target}_ensemble_{i}.pkl")
            MODELS[target].append(model)
        
        SCALERS[target] = joblib.load(f"{ARTIFACTS_PATH}/scaler_{target}.pkl")
        with open(f"{ARTIFACTS_PATH}/features_{target}.pkl", "rb") as f:
            FEATURES[target] = pickle.load(f)
        
        # Cria um explainer para cada modelo do ensemble
        EXPLAINERS[target] = [shap.TreeExplainer(m) for m in MODELS[target]]

    print("✅ Artefatos de ML e Explainers HÍBRIDOS carregados com sucesso.")
except Exception as e:
    print(f"❌ ERRO CRÍTICO ao carregar artefatos de ML: {e}")
    MODELS = None # Invalida para a verificação de saúde da API

# --- Funções de Pré-processamento ATUALIZADAS (Modelo Híbrido) ---

def preprocess_target1(df_input):
    """Pré-processamento específico para Target 1 (modelo único)"""
    df = df_input.copy()
    
    # Conversão de F0103
    if 'F0103' in df.columns: 
        df['F0103'] = pd.to_numeric(df['F0103'].astype(str).str.replace(',', '.'), errors='coerce')
    
    # Identificação de colunas
    p_cols = [c for c in df.columns if c.startswith('P') and any(char.isdigit() for char in c)]
    t_cols = [c for c in df.columns if c.startswith('T') and any(char.isdigit() for char in c)]
    f_cols = [c for c in df.columns if c.startswith('F') and len(c) > 1 and any(char.isdigit() for char in c)]
    
    # Engenharia de features - taxas de pulos
    p_minus_ones = sum((df[col] == -1).sum() for col in p_cols if col in df.columns)
    t_minus_ones = sum((df[col] == -1).sum() for col in t_cols if col in df.columns)
    df['taxa_pulos_P'] = p_minus_ones / len(p_cols) if len(p_cols) > 0 else 0
    df['taxa_pulos_T'] = t_minus_ones / len(t_cols) if len(t_cols) > 0 else 0
    df['taxa_pulos_geral'] = (p_minus_ones + t_minus_ones) / (len(p_cols) + len(t_cols)) if (len(p_cols) + len(t_cols)) > 0 else 0

    # Processamento de colunas numéricas
    for col in p_cols + t_cols + f_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').replace(-1, np.nan)
            df[col].fillna(df[col].median(), inplace=True)
            
    # Features de sono
    if 'QtdHorasDormi' in df.columns and 'Acordar' in df.columns:
        df['sono_total'] = df['QtdHorasDormi']
        df['sono_x_acordar'] = df['QtdHorasDormi'] * df['Acordar']
        df['sono_squared'] = df['QtdHorasDormi'] ** 2
        df['sono_irregular'] = np.abs(df['QtdHorasDormi'] - df['QtdHorasDormi'].median())

    # Estatísticas das colunas P
    if p_cols: 
        df['P_mean'] = df[p_cols].mean(axis=1)
        df['P_std'] = df[p_cols].std(axis=1)
        df['P_min'] = df[p_cols].min(axis=1)
        df['P_max'] = df[p_cols].max(axis=1)
        df['P_range'] = df['P_max'] - df['P_min']
        df['P_late'] = df[['P09', 'P12', 'P13', 'P15']].mean(axis=1) if all(c in df.columns for c in ['P09', 'P12', 'P13', 'P15']) else 0
        df['P_early'] = df[['P01', 'P02', 'P03', 'P04']].mean(axis=1) if all(c in df.columns for c in ['P01', 'P02', 'P03', 'P04']) else 0
    
    # Estatísticas das colunas T
    if t_cols: 
        df['T_mean'] = df[t_cols].mean(axis=1)
        df['T_std'] = df[t_cols].std(axis=1)
        df['T_min'] = df[t_cols].min(axis=1)
        df['T_max'] = df[t_cols].max(axis=1)
        
    # Features específicas das colunas F
    f_perfil = [c for c in f_cols if c.startswith('F01') or c.startswith('F02')]
    if f_perfil: 
        df['F_perfil_mean'] = df[f_perfil].mean(axis=1)
        df['F_perfil_std'] = df[f_perfil].std(axis=1)

    f_sono = [c for c in f_cols if c.startswith('F07')]
    if f_sono: 
        df['F_sono_mean'] = df[f_sono].mean(axis=1)
        df['F_sono_std'] = df[f_sono].std(axis=1)
    
    f_final = [c for c in f_cols if c.startswith('F11')]
    if f_final: 
        df['F_final_mean'] = df[f_final].mean(axis=1)
        df['F_final_std'] = df[f_final].std(axis=1)

    df['F_mean_geral'] = df[f_cols].mean(axis=1)

    # Interações entre as top 3 features
    top3 = [f for f in FEATURES['target1'] if '_X_' not in f][:3]
    for i, f1 in enumerate(top3):
        for f2 in top3[i+1:]:
            df[f'{f1}_X_{f2}'] = df.get(f1, 0) * df.get(f2, 0)
    
    # Garante todas as features esperadas pelo modelo
    df_final = df.reindex(columns=FEATURES['target1'], fill_value=0)
    return SCALERS['target1'].transform(df_final)

def preprocess_target2(df_input):
    """Pré-processamento específico para Target 2 (ensemble)"""
    df = df_input.copy()
    
    if 'F0103' in df.columns: 
        df['F0103'] = pd.to_numeric(df['F0103'].astype(str).str.replace(',', '.'), errors='coerce')
    
    # Identificação de colunas
    p_cols = [c for c in df.columns if c.startswith('P') and any(char.isdigit() for char in c)]
    t_cols = [c for c in df.columns if c.startswith('T') and any(char.isdigit() for char in c)]
    f_cols = [c for c in df.columns if c.startswith('F') and len(c) > 1 and any(char.isdigit() for char in c)]

    # Processamento de colunas numéricas
    for col in p_cols + t_cols + f_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').replace(-1, np.nan)
            df[col].fillna(df[col].median(), inplace=True)
            
    # Features básicas de sono
    if 'QtdHorasDormi' in df.columns and 'Acordar' in df.columns:
        df['sono_total'] = df['QtdHorasDormi']
        df['acordar'] = df['Acordar']

    # Médias específicas
    f_sono = [c for c in f_cols if c.startswith('F07')]
    if f_sono: 
        df['F_sono_mean'] = df[f_sono].mean(axis=1)

    f_final = [c for c in f_cols if c.startswith('F11')]
    if f_final: 
        df['F_final_mean'] = df[f_final].mean(axis=1)

    if p_cols: 
        df['P_mean'] = df[p_cols].mean(axis=1)
    
    # Interação entre as duas principais features
    base_features = [f for f in FEATURES['target2'] if '_X_' not in f]
    if len(base_features) >= 2:
        f1, f2 = base_features[0], base_features[1]
        interaction_name = f'{f1}_X_{f2}'
        if interaction_name in FEATURES['target2']:
            df[interaction_name] = df[f1] * df[f2]

    # Garante todas as features esperadas pelo modelo
    df_final = df.reindex(columns=FEATURES['target2'], fill_value=0)
    return SCALERS['target2'].transform(df_final)

def preprocess_target3(df_input):
    """Pré-processamento específico para Target 3 (ensemble)"""
    df = df_input.copy()

    if 'F0103' in df.columns: 
        df['F0103'] = pd.to_numeric(df['F0103'].astype(str).str.replace(',', '.'), errors='coerce')

    # Identificação de colunas
    p_cols = [c for c in df.columns if c.startswith('P') and any(char.isdigit() for char in c)]
    t_cols = [c for c in df.columns if c.startswith('T') and any(char.isdigit() for char in c)]
    f_cols = [c for c in df.columns if c.startswith('F') and len(c) > 1 and any(char.isdigit() for char in c)]

    # Processamento de colunas numéricas
    for col in p_cols + t_cols + f_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').replace(-1, np.nan)
            df[col].fillna(df[col].median(), inplace=True)
    
    # Estatísticas avançadas das colunas P
    if p_cols:
        df['P_mean'] = df[p_cols].mean(axis=1)
        df['P_std'] = df[p_cols].std(axis=1)
        df['P_late'] = df[['P09', 'P12', 'P13', 'P15']].mean(axis=1) if all(c in df.columns for c in ['P09', 'P12', 'P13', 'P15']) else 0
        df['P_early'] = df[['P01', 'P02', 'P03', 'P04']].mean(axis=1) if all(c in df.columns for c in ['P01', 'P02', 'P03', 'P04']) else 0

    # Estatísticas das colunas T
    if t_cols:
        df['T_mean'] = df[t_cols].mean(axis=1)
        df['T_std'] = df[t_cols].std(axis=1)

    # Features de sono avançadas
    if 'QtdHorasSono' in df.columns:
        f_sono = [c for c in f_cols if '07' in c]
        if f_sono:
            df['F_sono_mean'] = df[f_sono].mean(axis=1)
            df['F_sono_std'] = df[f_sono].std(axis=1)
            df['F_sono_max'] = df[f_sono].max(axis=1)
        if 'Acordar' in df.columns:
            df['sono_x_acordar'] = df['QtdHorasSono'] * df['Acordar']
            df['acordar_squared'] = df['Acordar'] ** 2
    
    # Features finais
    f_final = [c for c in f_cols if '11' in c]
    if f_final: 
        df['F_final_mean'] = df[f_final].mean(axis=1)

    # Interação específica para Target 3
    if 'F1103' in df.columns and 'P_mean' in df.columns and 'F1103_X_P_mean' in FEATURES['target3']:
        df['F1103_X_P_mean'] = df['F1103'] * df['P_mean']
    
    # Garante todas as features esperadas pelo modelo
    df_final = df.reindex(columns=FEATURES['target3'], fill_value=0)
    return SCALERS['target3'].transform(df_final)

# --- Rotas da API (ATUALIZADAS) ---
@app.get("/health", status_code=status.HTTP_200_OK)
def health_check():
    """Endpoint de health check para o Docker Compose."""
    if MODELS is None:
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="Modelos de ML não carregados.")
    return {"status": "ok"}

@app.post("/register", status_code=status.HTTP_201_CREATED)
def register(user: schemas.UserCreate, db: Session = Depends(database.get_db)):
    db_user = crud.get_user_by_username(db, username=user.username)
    if db_user:
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail="Usuário já existe")
    try:
        crud.create_user(db=db, user_schema=user)
        return {"msg": "Usuário registrado com sucesso"}
    except IntegrityError: # Captura erro de corrida (race condition)
        db.rollback()
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail="Usuário já existe")

@app.post("/login", response_model=schemas.Token)
def login(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(database.get_db)):
    user = crud.get_user_by_username(db, username=form_data.username)
    if not user or not auth.verify_password(form_data.password, user.password_hash):
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Credenciais inválidas")
    access_token = auth.create_access_token(data={"sub": str(user.id)})
    return {"access_token": access_token, "token_type": "bearer"}

@app.post("/predict")
async def predict(file: UploadFile = File(...), user_id: str = Depends(auth.get_current_user_id), db: Session = Depends(database.get_db)):
    if MODELS is None:
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="Modelos de ML não estão disponíveis.")
    
    try:
        contents = await file.read()
        buffer = BytesIO(contents)
        df_new = pd.read_excel(buffer)
        if 'Código de Acesso' not in df_new.columns:
            print("Coluna 'Código de Acesso' não encontrada no arquivo.")
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Coluna 'Código de Acesso' não encontrada no arquivo.")
    except Exception as e:
        print(f"Erro ao ler o arquivo Excel: {e}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"Erro ao ler o arquivo Excel: {e}")

    df_results = df_new.copy()
    shap_data = {}

    try:
        # Previsão T1 (modelo único)
        X_scaled_t1 = preprocess_target1(df_new)
        df_results['Previsão T1'] = MODELS['target1'].predict(X_scaled_t1).round(2)
        
        # Previsão T2 (ensemble - média dos 3 modelos)
        X_scaled_t2 = preprocess_target2(df_new)
        preds_t2 = [model.predict(X_scaled_t2) for model in MODELS['target2']]
        df_results['Previsão T2'] = np.mean(preds_t2, axis=0).round(2)
        
        # Previsão T3 (ensemble - média dos 3 modelos)
        X_scaled_t3 = preprocess_target3(df_new)
        preds_t3 = [model.predict(X_scaled_t3) for model in MODELS['target3']]
        df_results['Previsão T3'] = np.mean(preds_t3, axis=0).round(2)

        # Cálculo SHAP - ATUALIZADO PARA ENSEMBLE
        # T1 (modelo único)
        shap_values_t1 = EXPLAINERS['target1'].shap_values(X_scaled_t1)
        
        # T2 (média dos SHAP values dos 3 modelos do ensemble)
        shap_values_list_t2 = [explainer.shap_values(X_scaled_t2) for explainer in EXPLAINERS['target2']]
        shap_values_t2 = np.mean(shap_values_list_t2, axis=0)

        # T3 (média dos SHAP values dos 3 modelos do ensemble)
        shap_values_list_t3 = [explainer.shap_values(X_scaled_t3) for explainer in EXPLAINERS['target3']]
        shap_values_t3 = np.mean(shap_values_list_t3, axis=0)

        # Estrutura dos dados SHAP para resposta
        for i, j_id in enumerate(df_results['Código de Acesso']):
            shap_data[str(j_id)] = {
                'T1': {
                    'shap_values': shap_values_t1[i].tolist(), 
                    'feature_names': FEATURES['target1']
                },
                'T2': {
                    'shap_values': shap_values_t2[i].tolist(), 
                    'feature_names': FEATURES['target2']
                },
                'T3': {
                    'shap_values': shap_values_t3[i].tolist(), 
                    'feature_names': FEATURES['target3']
                }
            }
            
    except Exception as e:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Erro durante o pipeline de previsão: {e}")

    # Salvar no DB
    try:
        for _, row in df_results.iterrows():
            db.add(models.Prediction(
                user_id=int(user_id), 
                jogador_id=str(row['Código de Acesso']), 
                pred_t1=row['Previsão T1'], 
                pred_t2=row['Previsão T2'], 
                pred_t3=row['Previsão T3']
            ))
        db.commit()
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Erro ao salvar previsão no banco de dados: {e}")

    return {
        "predictions": df_results[['Código de Acesso', 'Previsão T1', 'Previsão T2', 'Previsão T3']].to_dict('records'),
        "shap_data": shap_data
    }

@app.get("/history")
def get_history(user_id: str = Depends(auth.get_current_user_id), db: Session = Depends(database.get_db)):
    query = db.query(
        models.Prediction.upload_timestamp, 
        func.count(models.Prediction.id).label('num_jogadores')
    ).filter(models.Prediction.user_id == int(user_id)).group_by(models.Prediction.upload_timestamp).order_by(models.Prediction.upload_timestamp.desc()).all()
    return [{"timestamp": r.upload_timestamp.strftime("%Y-%m-%d %H:%M:%S"), "num_jogadores": r.num_jogadores} for r in query]

@app.get("/feature_importance")
def get_feature_importance(user_id: str = Depends(auth.get_current_user_id)):
    if MODELS is None:
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="Modelos de ML não carregados.")
    
    importances_data = {}
    try:
        # Target 1 (modelo único)
        if hasattr(MODELS['target1'], 'feature_importances_'):
            df_imp_t1 = pd.DataFrame({
                'feature': FEATURES['target1'], 
                'importance': MODELS['target1'].feature_importances_
            }).sort_values(by='importance', ascending=False).head(20)
            importances_data['Target1'] = df_imp_t1.to_dict('records')
        else:
            importances_data['Target1'] = []

        # Targets 2 e 3 (média das importâncias dos ensembles)
        for target_key, target_name in [('target2', 'Target2'), ('target3', 'Target3')]:
            all_importances = []
            for model in MODELS[target_key]:
                if hasattr(model, 'feature_importances_'):
                    all_importances.append(model.feature_importances_)
            
            if all_importances:
                avg_importance = np.mean(all_importances, axis=0)
                df_imp = pd.DataFrame({
                    'feature': FEATURES[target_key],
                    'importance': avg_importance
                }).sort_values(by='importance', ascending=False).head(20)
                importances_data[target_name] = df_imp.to_dict('records')
            else:
                importances_data[target_name] = []
                
        return importances_data
    except Exception as e:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Erro ao calcular feature importance: {e}")
</file>

<file path="models.py">
# models.py
from sqlalchemy import Column, Integer, String, Float, DateTime, ForeignKey
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from database import Base

class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True, nullable=False)
    password_hash = Column(String, nullable=False)
    predictions = relationship('Prediction', backref='user', lazy=True)

class Prediction(Base):
    __tablename__ = 'predictions'
    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    upload_timestamp = Column(DateTime, nullable=False, default=func.now())
    jogador_id = Column(String, nullable=False)
    pred_t1 = Column(Float, nullable=False)
    pred_t2 = Column(Float, nullable=False)
    pred_t3 = Column(Float, nullable=False)
</file>

<file path="README.md">
# Backend API (FastAPI)

Esta API serve como o cérebro do projeto, lidando com autenticação, processamento de dados e previsões de Machine Learning.

## Principais Funcionalidades

-   **Autenticação JWT:** Sistema de registro (`/register`) e login (`/login`) que gera tokens JWT para proteger os endpoints.
-   **Endpoint de Previsão (`/predict`):** Recebe um arquivo `.xlsx` com novos dados de jogadores, aplica o mesmo pipeline de pré-processamento dos modelos treinados e retorna as previsões para os 3 targets.
-   **Análise SHAP:** Junto com as previsões, a API calcula os valores SHAP para cada jogador, permitindo entender a contribuição de cada feature para o resultado.
-   **Histórico de Previsões (`/history`):** Salva cada lote de previsões no banco de dados, associado ao usuário que fez o upload.
-   **Análise do Modelo (`/feature_importance`):** Expõe a importância geral das features para cada modelo.

## Principais Endpoints

-   `POST /register`: Cria um novo usuário.
-   `POST /login`: Autentica um usuário e retorna um token de acesso.
-   `POST /predict`: (Protegido) Recebe um arquivo Excel e retorna as previsões e dados SHAP.
-   `GET /history`: (Protegido) Retorna o histórico de uploads do usuário logado.
-   `GET /feature_importance`: (Protegido) Retorna a importância das features para cada modelo.
-   `GET /health`: Verifica a saúde da aplicação, incluindo o carregamento dos modelos de ML.
</file>

<file path="requirements.txt">
fastapi
uvicorn[standard]
python-multipart
sqlalchemy
psycopg2-binary
python-jose[cryptography]
pandas
scikit-learn==1.7.2 # Fixando a versão para consistência
joblib
openpyxl
catboost
shap
Flask-Bcrypt 
optuna
lightgbm
xgboost
reportlab
</file>

<file path="schemas.py">
# schemas.py
from pydantic import BaseModel

class UserCreate(BaseModel):
    username: str
    password: str

class Token(BaseModel):
    access_token: str
    token_type: str
</file>

</files>
